{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 Capstone - News Article Analysis & Classification\n",
    "\n",
    "## John A. Fonte\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Find 100 different entries from at least 10 different authors (articles?)\n",
    "2. Reserve 25% for test set\n",
    "3. cluster vectorized data (go through a few clustering methods)\n",
    "4. Perform unsupervised feature generation and selection\n",
    "5. Perform supervised modeling by classifying by author\n",
    "6. Comment on your 25% holdout group. Did the clusters for the holdout group change dramatically, or were they consistent with the training groups? Is the performance of the model consistent? If not, why?\n",
    "7. Conclude with which models (clustering or not) work best for classifying texts.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "__Source:__ https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#\n",
    "\n",
    "__Description:__ This is a subset of the [Reuters Corpus Volume 1 (RCV1)](https://scikit-learn.org/0.17/datasets/rcv1.html). Specifically, this subset consists of the top 50 authors by article proliferation, with a total of 100 articles per each author within the combined training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 1. Data Load and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "# will be doing other imports ad hoc\n",
    "##### i.e., models and related functions\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Data from Local Computer\n",
    "Each author is a subfolder, and within each folder is a series of .txt files\n",
    "The goal of this cell is to load all the contents of every subfolder into the \n",
    "DataFrame, while retaining the author designation for those works.\n",
    "'''\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "def multiple_file_load(file_directory):\n",
    "    \n",
    "    # identifying all author subfolders - appending them into list \n",
    "    \n",
    "    authorlist = []\n",
    "    textlist = []\n",
    "    \n",
    "    for author in listdir(file_directory):\n",
    "        authorname = str(author)\n",
    "        author_sub_directory = (file_directory + '/' + author) #author file path\n",
    "    \n",
    "    # identifying all files within each subfolder - \n",
    "    \n",
    "        for filename in listdir(author_sub_directory):\n",
    "            text_file_path = (author_sub_directory + '/' + filename) # text file path\n",
    "            \n",
    "            if (filename.lower().endswith('txt')):\n",
    "                authorlist.append(authorname)\n",
    "                textfile = open(text_file_path,'r') # this is how you open files\n",
    "                substantive_text = textfile.read()  # this is how to read a file\n",
    "                textlist.append(substantive_text)   # this is how to do something with that file\n",
    "                textfile.close()                    # this is how to close the file \n",
    "                                                             # (you must close one before opening another!)\n",
    "  # pushing the two lists into a dataframe \n",
    "\n",
    "    df = pd.DataFrame({'Author':authorlist, 'Text':textlist})\n",
    "    \n",
    "    return df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data (note the file path)\n",
    "df_train = multiple_file_load('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/C50/C50train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author                                               Text\n",
       "0  AaronPressman  The Internet may be overflowing with new techn...\n",
       "1  AaronPressman  The U.S. Postal Service announced Wednesday a ...\n",
       "2  AaronPressman  Elementary school students with access to the ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors don't have space between the names\n",
    "# adding the space in the authors...because I want it\n",
    "import re\n",
    "author_split = [re.findall('[A-Z][a-z]*', i) for i in df_train.Author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining them back together\n",
    "author_join = []\n",
    "\n",
    "for couple in author_split:\n",
    "    joined_string = couple[0] + ' ' + couple[1]\n",
    "    author_join.append(joined_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China issued tough new rules on the handling o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China will avoid bold moves in tackling its ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>Communist Party chief Jiang Zemin has put his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author                                               Text\n",
       "2497  William Kazer  China issued tough new rules on the handling o...\n",
       "2498  William Kazer  China will avoid bold moves in tackling its ai...\n",
       "2499  William Kazer  Communist Party chief Jiang Zemin has put his ..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Author'] = pd.Series(author_join)\n",
    "df_train.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>U.S. Senators on Tuesday sharply criticized a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>Two members of Congress criticised the Federal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>Commuters stuck in traffic on the Leesburg Pik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author                                               Text\n",
       "0  AaronPressman  U.S. Senators on Tuesday sharply criticized a ...\n",
       "1  AaronPressman  Two members of Congress criticised the Federal...\n",
       "2  AaronPressman  Commuters stuck in traffic on the Leesburg Pik..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading df_test, which is a separate csv file\n",
    "\n",
    "df_test = multiple_file_load('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/C50/C50test')\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another fix to Author column\n",
    "\n",
    "author_split = [re.findall('[A-Z][a-z]*', i) for i in df_test.Author]\n",
    "\n",
    "author_join = []\n",
    "\n",
    "for couple in author_split:\n",
    "    joined_string = couple[0] + ' ' + couple[1]\n",
    "    author_join.append(joined_string)    \n",
    "    \n",
    "df_test['Author'] = pd.Series(author_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before I begin adding features, assignment asks for 25% data split, NOT 50/50\n",
    "# See \"GOAL\" below for explanation as to how I am doing that.\n",
    "\n",
    "'''GOAL:\n",
    "Trying to get half of the datapoints OF EACH AUTHOR\n",
    "in the testing set into a new DataFrame, which\n",
    "will be concatenated onto the training set.\n",
    "I will delete that from the testing set later.\n",
    "\n",
    "Doing this instead of combining both and splitting 75/25 later \n",
    "ensures balanced data between the authors.\n",
    "'''\n",
    "\n",
    "def appendingdataframe(dataframe):\n",
    "    appendabledataframe = pd.DataFrame(columns=['Author', 'Text'])\n",
    "    \n",
    "    for item in dataframe.Author.unique():\n",
    "        df_testauthor = df_test[df_test['Author'] == item].copy() \n",
    "        appendabledataframe = appendabledataframe.append(df_testauthor[25:], \n",
    "                                                         ignore_index=True) # want half of df_testauthor!\n",
    "    \n",
    "    return appendabledataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through  Aaron Pressman\n",
      "Looping through  Alan Crosby\n",
      "Looping through  Alexander Smith\n",
      "Looping through  Benjamin Kang\n",
      "Looping through  Bernard Hickey\n",
      "Looping through  Brad Dorfman\n",
      "Looping through  Darren Schuettler\n",
      "Looping through  David Lawder\n",
      "Looping through  Edna Fernandes\n",
      "Looping through  Eric Auchard\n",
      "Looping through  Fumiko Fujisaki\n",
      "Looping through  Graham Earnshaw\n",
      "Looping through  Heather Scoffield\n",
      "Looping through  Jane Macartney\n",
      "Looping through  Jan Lopatka\n",
      "Looping through  Jim Gilchrist\n",
      "Looping through  Joe Ortiz\n",
      "Looping through  John Mastrini\n",
      "Looping through  Jonathan Birt\n",
      "Looping through  Jo Winterbottom\n",
      "Looping through  Karl Penhaul\n",
      "Looping through  Keith Weir\n",
      "Looping through  Kevin Drawbaugh\n",
      "Looping through  Kevin Morrison\n",
      "Looping through  Kirstin Ridley\n",
      "Looping through  Kourosh Karimkhany\n",
      "Looping through  Lydia Zajc\n",
      "Looping through  Lynne O\n",
      "Looping through  Lynnley Browning\n",
      "Looping through  Marcel Michelson\n",
      "Looping through  Mark Bendeich\n",
      "Looping through  Martin Wolk\n",
      "Looping through  Matthew Bunce\n",
      "Looping through  Michael Connor\n",
      "Looping through  Mure Dickie\n",
      "Looping through  Nick Louth\n",
      "Looping through  Patricia Commins\n",
      "Looping through  Peter Humphrey\n",
      "Looping through  Pierre Tran\n",
      "Looping through  Robin Sidel\n",
      "Looping through  Roger Fillion\n",
      "Looping through  Samuel Perry\n",
      "Looping through  Sarah Davison\n",
      "Looping through  Scott Hillis\n",
      "Looping through  Simon Cowell\n",
      "Looping through  Tan Ee\n",
      "Looping through  Therese Poletti\n",
      "Looping through  Tim Farrand\n",
      "Looping through  Todd Nissen\n",
      "Looping through  William Kazer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using appendabledataframe to avoid screwing up original data\n",
    "# This is explicit inefficiency at the cost of being cautious\n",
    "\n",
    "df_train2 = df_train.append(appendingdataframe(df_train), ignore_index=True)\n",
    "\n",
    "# checking if the appending worked\n",
    "len(df_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It worked!\n",
    "df_train = df_train2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through  Aaron Pressman\n",
      "Looping through  Alan Crosby\n",
      "Looping through  Alexander Smith\n",
      "Looping through  Benjamin Kang\n",
      "Looping through  Bernard Hickey\n",
      "Looping through  Brad Dorfman\n",
      "Looping through  Darren Schuettler\n",
      "Looping through  David Lawder\n",
      "Looping through  Edna Fernandes\n",
      "Looping through  Eric Auchard\n",
      "Looping through  Fumiko Fujisaki\n",
      "Looping through  Graham Earnshaw\n",
      "Looping through  Heather Scoffield\n",
      "Looping through  Jane Macartney\n",
      "Looping through  Jan Lopatka\n",
      "Looping through  Jim Gilchrist\n",
      "Looping through  Joe Ortiz\n",
      "Looping through  John Mastrini\n",
      "Looping through  Jonathan Birt\n",
      "Looping through  Jo Winterbottom\n",
      "Looping through  Karl Penhaul\n",
      "Looping through  Keith Weir\n",
      "Looping through  Kevin Drawbaugh\n",
      "Looping through  Kevin Morrison\n",
      "Looping through  Kirstin Ridley\n",
      "Looping through  Kourosh Karimkhany\n",
      "Looping through  Lydia Zajc\n",
      "Looping through  Lynne O\n",
      "Looping through  Lynnley Browning\n",
      "Looping through  Marcel Michelson\n",
      "Looping through  Mark Bendeich\n",
      "Looping through  Martin Wolk\n",
      "Looping through  Matthew Bunce\n",
      "Looping through  Michael Connor\n",
      "Looping through  Mure Dickie\n",
      "Looping through  Nick Louth\n",
      "Looping through  Patricia Commins\n",
      "Looping through  Peter Humphrey\n",
      "Looping through  Pierre Tran\n",
      "Looping through  Robin Sidel\n",
      "Looping through  Roger Fillion\n",
      "Looping through  Samuel Perry\n",
      "Looping through  Sarah Davison\n",
      "Looping through  Scott Hillis\n",
      "Looping through  Simon Cowell\n",
      "Looping through  Tan Ee\n",
      "Looping through  Therese Poletti\n",
      "Looping through  Tim Farrand\n",
      "Looping through  Todd Nissen\n",
      "Looping through  William Kazer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing same for df_test\n",
    "\n",
    "df_test2 = df_test.append(appendingdataframe(df_train), ignore_index=True)\n",
    "\n",
    "# checking if the appending worked\n",
    "len(df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now to drop the rows added to df_train from df_test\n",
    "\n",
    "df_test2.drop_duplicates(keep=False, inplace=True)\n",
    "len(df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "Arguably the most important part about working with text data is how to refine it for processing. As simple as they are, string substitutions such as `pd.replace` and regex's `re.sub` are common. I am also partial to `pd.Series.apply(lambda x: x.replace('...',''))`.  Additional text processing such as the exclusion of stop_words and lemmatization will be done after the raw text is pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text before feature analysis/engineering \n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "# CLEANING FUNCTION 1 - WORD AND PUNCTUATION/CHARACTER CLEANING\n",
    "\n",
    "# EDIT: During first run-through, this function was very basic\n",
    "# I have since implemented new cleaning features\n",
    "\n",
    "# regex already imported as re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    text = text.lower() # avoiding capitalization problems.\n",
    "    \n",
    "    text = re.sub(r'.\\s*\\\\n[a-z]', r'\\. [a-z]', text)\n",
    "    text = re.sub(r'\\.\\s?([a-z])', r'\\. \\1', text)\n",
    "    text = re.sub(r' u\\. s\\.(\\s?)', r' u\\.s\\.\\1', text) # next three lines are my attempt to join 'u. s.' to 'u.s.'\n",
    "    text = text.replace('u. s.', 'u.s.')\n",
    "    text = text.replace(r'u. s.', r'u.s.')\n",
    "    text = re.sub(r'-', '', text)\n",
    "    text = re.sub(r'  ', ' ', text)\n",
    "    text = re.sub('[\\[].*?[\\]]', '', text)\n",
    "    text = re.sub('.=.', '. .', text)\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = re.sub(',', '', text) # I don't want punct screwing up lemmatization\n",
    "    text = re.sub('\\\\n', '', text)\n",
    "    text = re.sub(r'\\\\n', '', text) # I don't know which one works\n",
    "    text = text.replace('\\\"', '')\n",
    "    \n",
    "    # rest of punctuation will be handled via lemmatization\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application of text cleaning functions\n",
    "\n",
    "df_train['Text1'] = df_train['Text'].apply(lambda x: text_cleaner(x))\n",
    "df_test['Text1'] = df_test['Text'].apply(lambda x: text_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the u. s. postal service announced wednesday a plan to boost online commerce by enhancing the security and reliability of electronic mail traveling on the internet. under the plan businesses and consumers can verify that email has not been tampered with and use services now available for ordinary mail like sending a certified letter.the leap from trading messages to buying and selling goods has been blocked by the fear of security threats robert reisner vice president of stategic planning said. to expand from local area networks and bilateral secure communications to wide use of electronic commerce will require a new generation of security services reisner said. cylink corp is developing a system for the post office to use to verify the identity of email senders. the system will enable people to register a digital signature with the post office that can be compared against electronic mail they send. if any tampering is discovered the postal service would investigate just like it investigates tampering with regular mail reisner said. the post office is also using the internet to enhance nonelectronic mail delivery. in conjunction with sun microsystems the postal service has also developed a new online system for bulk mailers that relies on sun's java computer language reisner said. instead of manually calculating postage rates for bulk mailings possibly using outdated forms a bulk mailer will be able to add up the charges using a software application written with java and posted on the internet. eventually the post office plans to offer hybrid services involving both kinds of mail reisner said. electronic mail could be converted to paper and delivered within one business day for example. or someone who moves could register a change of address with the post office over the internet. critics of the post office's internet plans have said the service is usurping functions that could be better performed by the private sector. but reisner rejected those arguments maintaining that the post office has often facilitated the spread of new technologies such as railroads and aircraft. remember the pilots who returned from world war i and then risked their lives in an even more dangerous duty resiner said refering to early airmail delivery efforts.to keep these daring pilots alive the postal service had to create parts of what we know as the weather service today. navigation aids had to be developed. we are watching many of these same dynamics today. earlier experience tells us that this is a public and private job.2028988312\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Text1'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING FUNCTION 2 - NUMBER CLEANING\n",
    "'''\n",
    "Whether it be phone numbers, page numbers, or just \n",
    "digits for no particular reason (which, yes, does happen),\n",
    "numbers will become there own vectors and inevitably clog\n",
    "up the vectorized feature space.\n",
    "\n",
    "I am dropping all number columns because \n",
    "we are looking at words, not numbers!!!!!\n",
    "'''\n",
    "\n",
    "def phone_and_weird_num_deletion(text):\n",
    "    final_text = text\n",
    "    \n",
    "    leading_zero_numbers = list(re.findall(r' 0\\d+', text))\n",
    "    phone_numbers = list(re.findall(r' ?\\+?\\d{1}?(\\d{3}?|\\d{4}?) \\d{3} \\d{4}', text))\n",
    "    phone_num_no_space = list(re.findall(r' ?\\+?(\\d{10}?|\\d{11}?)', text))\n",
    "    total_deletions = leading_zero_numbers + phone_numbers + phone_num_no_space\n",
    "    \n",
    "    if len(total_deletions) != 0:\n",
    "        print(len(total_deletions))\n",
    "        for item in total_deletions:\n",
    "            final_text = final_text.replace(item, '')\n",
    "        return final_text\n",
    "    \n",
    "    else:\n",
    "        return text\n",
    "                               \n",
    "#----------------------------------------------\n",
    "        \n",
    "# DON'T USE THE BELOW FUNCTION!\n",
    "# It appears there are a lot of good numbers there\n",
    "\n",
    "#-----------------------------------------------\n",
    "\n",
    "#def string_num_deletion(text):\n",
    "#    final_text = text\n",
    "#    \n",
    "#    dates = [str(x) for x in list(range(1900, 2026))]\n",
    "#    plural_dates = re.findall(r' (\\d{2}?|\\d{4}?)s/g', text)\n",
    "#    money_values = re.findall(r' \\$\\d+/g', text)\n",
    "#    common_num = list(range(1000))\n",
    "#    total_exceptions = dates + plural_dates + money_values + common_num\n",
    "#                              \n",
    "#    \n",
    "#    num_token = re.findall(r'\\d+[^snrt]?[^tdh]?\\.*\\d*/g', text) # finding all numbers that are not ordinals\n",
    "#                                                                # also using \".\" for decimal findings\n",
    "#    if len(num_token) != 0:\n",
    "#        for x in num_token not in total_exceptions:\n",
    "#            final_text = num_token.apply(lambda x: final_text.replace(x, ''))\n",
    "#        \n",
    "#        return final_text\n",
    "#    \n",
    "#    else:\n",
    "#        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "df_train['Text2'] = df_train['Text1'].apply(lambda x: phone_and_weird_num_deletion(x))\n",
    "df_test['Text2'] = df_test['Text1'].apply(lambda x: phone_and_weird_num_deletion(x))\n",
    "\n",
    "# The list comprehension inside a loop makes this run for a total of 10seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>the internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>the u. s. postal service announced wednesday a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author                                               Text\n",
       "0  Aaron Pressman  the internet may be overflowing with new techn...\n",
       "1  Aaron Pressman  the u. s. postal service announced wednesday a...\n",
       "2  Aaron Pressman  elementary school students with access to the ..."
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding cleaned text back into original text\n",
    "df_train['Text'] = df_train['Text2']\n",
    "df_test['Text'] = df_test['Text2']\n",
    "\n",
    "df_train.drop(columns=['Text1', 'Text2'], axis=1, inplace=True)\n",
    "df_test.drop(columns=['Text1', 'Text2'], axis=1, inplace=True)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Adding Features\n",
    "\n",
    "Now that the text is cleaned, we can calculate basic numerical features off of that cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some numerical features for text analysis\n",
    "\n",
    "df_train['Raw Character Count'] = df_train['Text'].apply(lambda x: len(x))\n",
    "df_train['Raw Word Count'] = df_train['Text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing same for df_test\n",
    "\n",
    "df_test['Raw Character Count'] = df_test['Text'].apply(lambda x: len(x))\n",
    "df_test['Raw Word Count'] = df_test['Text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating numerical classes for authors:\n",
    "# I feel like one hot encoding would've screwed things up, so I did \"factorize\"\n",
    "\n",
    "df_train['AuthorNum'] = pd.factorize(df_train.Author)[0]\n",
    "df_train['AuthorNum'] = df_train['AuthorNum'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and same for df_test...\n",
    "\n",
    "df_test['AuthorNum'] = pd.factorize(df_test.Author)[0]\n",
    "df_test['AuthorNum'] = df_test['AuthorNum'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing! Changing Text to Numbers\n",
    "\n",
    "Our text is ready to be processed. How are we going to do that? __Vectorization.__ This process turns words into vectors. These vectors are the values that we use for modeling.\n",
    "\n",
    "Sklearn offers two popular vectorizing models (although there are many others), _Bag of Words_ (\"CountVectorizer\") and _Term Frequency-Inverse Document Frequency_ (\"TfidfVectorizer\"). Bag of words vectors represent raw counts of each vectorized word found in a given article, and tfidf vectors represent inverse ratios of the word's frequency in a given article.\n",
    "\n",
    "### Okay cool...so what are we adding to the vectorizers?\n",
    "\n",
    "These two sklearn models vectorize _individual words_ only. There are more sophisticated models that can vectorize whole sentences at once. If you want to use count or tfidf for sentences, we can __tokenize__ the articles into sentences, which are broken into their own individual dataframe index values, and then use one of the two vectorizers to analyze the individual words in those sentences. We will not do that here: the articles are not uniform size, and so tokenizing the articles into words would imbalance the data. It also will use an unnecessaryamount of resources to do this.\n",
    "\n",
    "Note that ___sklearn models do the tokenization___! If you choose to tokenize yourself via the spaCy module, you will have to join back the text and then _re-tokenize_ it through the vectorizer.  We may need to do this instead of easily pushing in the raw (but cleaned) text documents; it appears that sklearn vectorizers do not have any native lemmatization hyperparameters (believe me, I looked..._a lot..._), and I believe lemmatization is crucial for analysis.\n",
    "\n",
    "From here we will do three things: run raw text documentation through (1) countvectorizer and (2) tfidfvectorizer. Separately, we will (3) lemmatize and tokenize the text with the spaCy module, and once that is done, rejoin the lemmatized tokens into raw text which we will push through tfidfvectorizer. We will analyze the data via __Latent Semantic Analysis__, where we will choose the best dataset to work with (and in turn, determine which way to df_test dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - COUNTVECTORIZER\n",
    "\n",
    "# importing model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiating model\n",
    "\n",
    "# using default values for vectorizer's \"preprocessor\" and \"tokenizer\"\n",
    "cv = CountVectorizer(ngram_range=(1, 1),   # this is default, but just showing that we're using individual words\n",
    "                     stop_words='english', # excluding stopwords\n",
    "                     min_df=5,             # HUGE parameter - excluding all words not appearing more than 5 times!\n",
    "                     max_features=12500,   # keeping a limit on how many features there should be\n",
    "                    )                      # max_features takes TOP frequency features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 12500)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizing only the training data, as requested\n",
    "\n",
    "count_vectors = cv.fit_transform(df_train['Text'])\n",
    "#dense = tfidf_vectors.todense() ----- for a feature space of this size\n",
    "#denselist = dense.tolist()     ------ fixing sparsity will crash/run out of memory\n",
    "\n",
    "token_names = cv.get_feature_names()\n",
    "\n",
    "# creating dataframe from vectors (a separate process)\n",
    "count_vectorized_df_train = pd.DataFrame(count_vectors.toarray(), columns=token_names)\n",
    "count_vectorized_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '0p', '10',\n",
       "       '100', '1000', '10000', '100000', '101', '102', '103', '104'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorized_df_train.columns[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I have no idea why the preceding 0 numbers weren't taken out\n",
    "of the text - they are now made into vectors. I am manually dropping those vectors.\n",
    "'''\n",
    "\n",
    "count_vectorized_df_train = count_vectorized_df_train.iloc[:, 12:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------\n",
    "# STEP 2 - TfidfVECTORIZER\n",
    "\n",
    "# doing same and comparing with countvec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# instantiating model\n",
    "\n",
    "# using default values for vectorizer's \"preprocessor\" and \"tokenizer\"\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1),   # this is default, but just showing that we're using individual words\n",
    "                        stop_words='english', # excluding stopwords\n",
    "                        min_df=5,             # HUGE parameter - excluding all words not appearing more than 2 times\n",
    "                        max_features=12500,    # tfidf goes much faster than countvec\n",
    "                       )                      # max_features takes TOP frequency features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 12500)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors = tfidf.fit_transform(df_train['Text'])\n",
    "#dense = tfidf_vectors.todense() ----- for a feature space of this size\n",
    "#denselist = dense.tolist()     ------ fixing sparsity will crash/run out of memory\n",
    "\n",
    "token_names = tfidf.get_feature_names()\n",
    "\n",
    "# creating dataframe from vectors (a separate process)\n",
    "tfidf_vectorized_df_train = pd.DataFrame(tfidf_vectors.toarray(), columns=token_names)\n",
    "tfidf_vectorized_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '0p', '10',\n",
       "       '100', '1000', '10000', '100000', '101', '102', '103', '104'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorized_df_train.columns[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorized_df_train = tfidf_vectorized_df_train.iloc[:,12:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difference between the two?\n",
    "\n",
    "There really isn't: the output vectors represent the same thing. The difference is that countvec has raw number vectors, and tfidf is the log inverse frequency of those counts.\n",
    "\n",
    "__Which one should we use for modeling?:__\n",
    "\n",
    "The answer is __tfidf vectors__. The inverse frequency offers the benefit of normalizing the data, which is critical for clustering purposes.\n",
    "\n",
    "We will now do a third vectorization with out spaCy lemmatization and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing spaCy, which will allow for lemmatization\n",
    "import spacy\n",
    "\n",
    "# WILL TAKE A WHILE TO LOAD!\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "df_train['Spacy-ed Text'] = df_train['Text'].apply(lambda text: nlp(text))\n",
    "df_test['Spacy-ed Text'] = df_test['Text'].apply(lambda text: nlp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Raw Character Count</th>\n",
       "      <th>Raw Word Count</th>\n",
       "      <th>AuthorNum</th>\n",
       "      <th>Spacy-ed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>the internet may be overflowing with new techn...</td>\n",
       "      <td>1976</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>(the, internet, may, be, overflowing, with, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>the u. s. postal service announced wednesday a...</td>\n",
       "      <td>2557</td>\n",
       "      <td>414</td>\n",
       "      <td>0</td>\n",
       "      <td>(the, u., s., postal, service, announced, wedn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>elementary school students with access to the ...</td>\n",
       "      <td>482</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>(elementary, school, students, with, access, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author                                               Text  \\\n",
       "0  Aaron Pressman  the internet may be overflowing with new techn...   \n",
       "1  Aaron Pressman  the u. s. postal service announced wednesday a...   \n",
       "2  Aaron Pressman  elementary school students with access to the ...   \n",
       "\n",
       "   Raw Character Count  Raw Word Count AuthorNum  \\\n",
       "0                 1976             319         0   \n",
       "1                 2557             414         0   \n",
       "2                  482              71         0   \n",
       "\n",
       "                                       Spacy-ed Text  \n",
       "0  (the, internet, may, be, overflowing, with, ne...  \n",
       "1  (the, u., s., postal, service, announced, wedn...  \n",
       "2  (elementary, school, students, with, access, t...  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining lemmatization model\n",
    "\n",
    "def lemma_list(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemmas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_) # this is why we needed to spacy/nlp-ify the texts first\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return lemmas\n",
    "\n",
    "def lemmatized_text(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemmas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_) # this is why we needed to spacy/nlp-ify the texts first\n",
    "    \n",
    "    lemma_text = ' '.join(lemmas)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return lemma_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing third model with lemma text\n",
    "\n",
    "# instantiating model\n",
    "\n",
    "# using default values for vectorizer's \"preprocessor\" and \"tokenizer\"\n",
    "tfidflemma = TfidfVectorizer(preprocessor=lemmatized_text,\n",
    "                             ngram_range=(1, 1),   # this is default, but just showing that we're using individual words\n",
    "                             stop_words='english', # excluding stopwords\n",
    "                             min_df=5,             # HUGE parameter - excluding all words not appearing more than 2 times\n",
    "                             max_features=12500,    # tfidf goes much faster than countvec\n",
    "                            )                      # max_features takes TOP frequency features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 10854)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidflemma_vectors = tfidflemma.fit_transform(df_train['Spacy-ed Text'])\n",
    "token_names = tfidflemma.get_feature_names()\n",
    "\n",
    "# creating dataframe from vectors (a separate process)\n",
    "tfidflemma_vectorized_df_train = pd.DataFrame(tfidflemma_vectors.toarray(), columns=token_names)\n",
    "tfidflemma_vectorized_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['00', '01', '02', '0200', '025', '03', '04', '05', '0500', '06', '07',\n",
       "       '0700', '08', '09', '0p', '10', '100', '1000', '10000', '100000'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidflemma_vectorized_df_train.columns[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidflemma_vectorized_df_train = tfidflemma_vectorized_df_train.iloc[:,16:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis:__\n",
    "\n",
    "Looking at the shapes between tfidf and the lemmatized tfidf, the feature space for lemmatized tfidf is smaller, which means (1) less computational overhead and (2) less variance loss when performing dimensionality reduction.\n",
    "\n",
    "For this reason: __we will be using the DataFrame tfidflemma_vectorized_df_train for modeling moving forward.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 5825)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing the same for df_test, to save for later:\n",
    "\n",
    "tfidflemma_vectors = tfidflemma.fit_transform(df_test['Spacy-ed Text'])\n",
    "token_names = tfidflemma.get_feature_names()\n",
    "\n",
    "# creating dataframe from vectors (a separate process)\n",
    "tfidflemma_vectorized_df_test = pd.DataFrame(tfidflemma_vectors.toarray(), columns=token_names)\n",
    "tfidflemma_vectorized_df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidflemma_vectorized_df_test = tfidflemma_vectorized_df_test.iloc[:,11:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# A Quick Aside:\n",
    "\n",
    "It is important to keep tabs of all the kinds of data that has so far been created, transformed, and/or deleted. Specifically, we now have two sets of datasets: raw text datasets (`df_train` and `df_test`) and the various vectorized datasets (e.g., `tfidflemma_vectorized_df_train`, with df_test to be vectorized as well).\n",
    "\n",
    "Before we continue, let's see these datasets and save them to csv's, so we don't have to do the analysis all over again.^\n",
    "\n",
    "---\n",
    "^From experience, when the kernel is reset, pandas cannot properly import the spaCy-ed data because spaCy is not imported at that time. This means you would have to spaCy the text data all over again even though you are importing a dataframe with the spaCy transformed data. It doesn't make much sense, since the transformation calculation is complete and we are just saving the outputs, but I don't question the code gods. Just to be safe though: __IMPORT SPACY _BEFORE_ YOU IMPORT THE DATAFRAME!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little out of order, but adding this additional feature, just for fun:\n",
    "df_train['Meaningful Word Count'] = df_train['Spacy-ed Text'].apply(lambda x: len(lemma_list(x)))\n",
    "df_test['Meaningful Word Count'] = df_test['Spacy-ed Text'].apply(lambda x: len(lemma_list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframes via pd.to_csv!!!!!!\n",
    "# commented out because I only need to do this once\n",
    "\n",
    "#df_train.to_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/NEW_COMPLETE_NLP-train.csv',\n",
    "#                index=False)\n",
    "#\n",
    "#df_test.to_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/NEW_COMPLETE_NLP-test.csv',\n",
    "#               index=False)\n",
    "#\n",
    "#tfidflemma_vectorized_df_train.to_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/VECTORIZED-train.csv',\n",
    "#               index=False)\n",
    "#\n",
    "#tfidflemma_vectorized_df_test.to_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/VECTORIZED-test.csv',\n",
    "#               index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Analysis: Latent Semantic Analysis (LSA)\n",
    "\n",
    "As mentioned before, vectorized data is extremely sparse (although for my vectorizer, I kinda forced it not to be too sparse via the hyperparameter `min_df=5`). Performing machine learning across thousands of features is computationally heavy, and likely not all that effective in explaining variance.  We need to employ a dimensionality reduction technique.\n",
    "\n",
    "For text data, the proper dimensionality reduction algorithm is __Singular Value Decomposition (SVD).__ Here, vectors are reduced down and compared with each other, yielding components. This is better than using PCA because: (1) SVD works better with sparse datasets than PCA does, and (2) PCA works on a normal distribution, which is good for continuous data, but terrible for non-continuous data (cough text data cough). Using SVD for dimensionality reduction is crucial for clustering algorithms and general modeling.\n",
    "\n",
    "__Latent Semantic Analysis (LSA)__ uses SVD on sparse text data to generate components that best \"cluster\" datapoints based on similarity, whether that similarity be the raw words, topic, order of words, etc. For all intents and purposes, SVD and LSA are used synonymously.\n",
    "\n",
    "---\n",
    "The downside to reducing the feature space down is that some explained variance is lost. Accordingly, we need to determine what is the best tradeoff between feature space simplicity for modeling and accuracy by retention of explained variance. How do we determine this? __Well, we kinda test it...__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing SVD\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to check varying component sizes (see bottom)\n",
    "component_datapoints = [5, 10, 50, 100, 250, 500, 1000, 2000, 3500, 5000]\n",
    "component_log = [np.log(x) for x in component_datapoints]\n",
    "\n",
    "# finding y values for SVD analysis graph\n",
    "variance_explained_percent = []\n",
    "\n",
    "# running loop on this\n",
    "\n",
    "for i in component_datapoints:\n",
    "    svd = TruncatedSVD(i)                                           # SVD instantiation\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False))                # model instantiation\n",
    "    X_train_lsa = lsa.fit_transform(tfidflemma_vectorized_df_train) # fit model\n",
    "    \n",
    "    variance_explained = svd.explained_variance_ratio_\n",
    "    total_variance = variance_explained.sum()\n",
    "    \n",
    "    variance_explained_percent.append(total_variance*100)\n",
    "    \n",
    "# -----------------------------------------------------------------\n",
    "'''\n",
    "This loop was VERY VERY VERY VERY computationally heavy. Took forever!\n",
    "For future reference so you don't have to re-run, \n",
    "\n",
    "variance_explained_percent = \n",
    "[5.42, 9.41, 23.86, 33.06, 49.03, \n",
    "64.11, 80.56, 94.80, 99.98, 100.00]\n",
    "'''\n",
    "a='hi' # setting this here so the above string isn't outputted when run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(variance_explained_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log(Number of Components)</th>\n",
       "      <th>Percent Variance Captured by Totality of Components</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.609438</td>\n",
       "      <td>5.421886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.302585</td>\n",
       "      <td>9.412167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.912023</td>\n",
       "      <td>23.862891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log(Number of Components)  \\\n",
       "0                   1.609438   \n",
       "1                   2.302585   \n",
       "2                   3.912023   \n",
       "\n",
       "   Percent Variance Captured by Totality of Components  \n",
       "0                                           5.421886    \n",
       "1                                           9.412167    \n",
       "2                                          23.862891    "
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up graphing data, because seaborn HATES lists!\n",
    "seaborndf = pd.DataFrame({'log(Number of Components)': component_log})\n",
    "seaborndf['Percent Variance Captured by Totality of Components'] = variance_explained_percent\n",
    "seaborndf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graphing SVD analysis\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "sns.lineplot(x='log(Number of Components)', \n",
    "             y='Percent Variance Captured by Totality of Components', \n",
    "             data=seaborndf,\n",
    "            markers=component_log)\n",
    "plt.title('Amount of Variance Retained through Varying Component Sizes via SVD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis:__ Based on this logarithmic analysis, you have to pile on the components to retain variance. Judging by the values found in `variance_explained_percent`, when compared to `component_datapoints`, __580 components__, or approximately 10% of the feature space, sounds good to me for the following:\n",
    "\n",
    "---\n",
    "---\n",
    "# Table of Contents for Remainder of Project\n",
    "\n",
    "1. Setting up X Input Data for Modeling\n",
    "    - Creating SVD-Truncated feature space\n",
    "    - Create separate 2D SVD-Truncated feature space for graphing\n",
    "    - Perform LSA unsupervised feature analysis (separate from unsupervised clustering)\n",
    "    - Add and scale aggregate counts to add to X feature space\n",
    "        - This is explicitly done _after_ one K-Means iteration analysis without those features\n",
    "    - Separate target variable from input data\n",
    "    - Do all of the above for testing feature space too\n",
    "<br> <br>\n",
    "2. Unsupervised Data Modeling\n",
    "    - K-Means clustering\n",
    "        - K-Elbow analysis\n",
    "        - Mini-batch sampling\n",
    "    - Mean-shift clustering\n",
    "    - t-SNE clustering\n",
    "    - Analysis and comparison\n",
    "<br> <br>\n",
    "3. Unsupervised Data Modeling for Testing Set\n",
    "    - Same unsupervised modeling as above\n",
    "    - Analysis and comparison/contrast with training set\n",
    "<br> <br>\n",
    "4. Dataset Preparation for Supervised Modeling\n",
    "    - Creation of supervised modeling dataset (X, y)\n",
    "    - Scaling and Transformation of feature space\n",
    "<br> <br>\n",
    "5. Supervised Classification Modeling\n",
    "    - Logistic Regression with LASSO and Ridge penalties\n",
    "    - K-Nearest Neighbors\n",
    "    - Random Forest\n",
    "    - AdaBoosting Ensemble Model\n",
    "<br> <br>\n",
    "6. Final Analysis\n",
    "    - Discussion of best models and shortcomings\n",
    "    \n",
    "---\n",
    "---\n",
    "\n",
    "# 1. Setting up X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by the totality of the components for the TRAINING dataset:  67.594\n",
      "\n",
      "Percent variance captured by the totality of the components for the TESTING dataset:  88.379\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: SETTING UP DIMENSION-REDUCED DATA\n",
    "\n",
    "# Set up vectorized LSA for input and unsupervised LSA analysis\n",
    "\n",
    "# although same dimensionality reduction, \n",
    "# making two pipelines to get two different variance ratios (see bottom of cell)\n",
    "svd1=TruncatedSVD(580)\n",
    "svd2=TruncatedSVD(580)\n",
    "\n",
    "lsa1 = make_pipeline(svd1, Normalizer(copy=False))\n",
    "lsa2 = make_pipeline(svd2, Normalizer(copy=False))\n",
    "\n",
    "X_train_lsa = lsa1.fit_transform(tfidflemma_vectorized_df_train)\n",
    "\n",
    "X_test_lsa = lsa2.fit_transform(tfidflemma_vectorized_df_test)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "'''\n",
    "While 580 components is a good reduced feature space \n",
    "for our vectorized datasets, we need to graph in 2D,\n",
    "(we *can* do 3D, but it's not necessary for this kind of data).\n",
    "Therefore, we need to truncate again into 2 dimensions for purposes of graphing\n",
    "'''\n",
    "\n",
    "svd_graph=TruncatedSVD(2)\n",
    "lsa_graph = make_pipeline(svd_graph, Normalizer(copy=False))\n",
    "\n",
    "X_train_lsa_graph = lsa_graph.fit_transform(tfidflemma_vectorized_df_train)\n",
    "X_test_lsa_graph = lsa_graph.fit_transform(tfidflemma_vectorized_df_test)\n",
    "\n",
    "#--------------------------------------------------------\n",
    "\n",
    "variance_explained_train = svd1.explained_variance_ratio_\n",
    "total_variance_train = variance_explained_train.sum()\n",
    "\n",
    "variance_explained_test = svd2.explained_variance_ratio_\n",
    "total_variance_test = variance_explained_test.sum()\n",
    "\n",
    "print(\"Percent variance captured by the totality of the components for the TRAINING dataset: \",\n",
    "      round(total_variance_train*100, 4))\n",
    "\n",
    "print(\"\\nPercent variance captured by the totality of the components for the TESTING dataset: \",\n",
    "      round(total_variance_test*100, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 580)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lsa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing additional features found in original dataframes\n",
    "# Only adding 2 features - the raw character count and the raw word count\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "norm = Normalizer() # instantiate scaler\n",
    "\n",
    "# applying the model\n",
    "character_count_normalized_train = norm.fit_transform(df_train['Raw Character Count'].values.reshape(-1,1))\n",
    "word_count_normalized_train = norm.fit_transform(df_train['Raw Word Count'].values.reshape(-1,1))\n",
    "\n",
    "character_count_normalized_test = norm.fit_transform(df_test['Raw Character Count'].values.reshape(-1,1))\n",
    "word_count_normalized_test = norm.fit_transform(df_test['Raw Word Count'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ I won't be adding these into the official X just yet. I want to compare the accuracies of X_lsa first without influence from the aggregate features.\n",
    "\n",
    "---\n",
    "\n",
    "### LSA Analysis\n",
    "\n",
    "We are able to gain some insight into which input LSA components provide the greatest explanation of variance in the dataset, and which authors' variances are being explained. This is just preliminary analysis before delving into the real unsupervised machine learning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Component 0 for Training Set:\n",
      "\n",
      "Author\n",
      "Mark Bendeich      0.649276\n",
      "Graham Earnshaw    0.632674\n",
      "Eric Auchard       0.630504\n",
      "Bernard Hickey     0.624931\n",
      "Bernard Hickey     0.621308\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1 for Training Set:\n",
      "\n",
      "Author\n",
      "Peter Humphrey    0.743966\n",
      "Peter Humphrey    0.710732\n",
      "William Kazer     0.706919\n",
      "Tan Ee            0.703369\n",
      "Peter Humphrey    0.699339\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2 for Training Set:\n",
      "\n",
      "Author\n",
      "David Lawder    0.561701\n",
      "David Lawder    0.556036\n",
      "David Lawder    0.554305\n",
      "David Lawder    0.553527\n",
      "David Lawder    0.551263\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3 for Training Set:\n",
      "\n",
      "Author\n",
      "David Lawder    0.598727\n",
      "David Lawder    0.595361\n",
      "David Lawder    0.585853\n",
      "David Lawder    0.583259\n",
      "David Lawder    0.583176\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4 for Training Set:\n",
      "\n",
      "Author\n",
      "Sarah Davison    0.374223\n",
      "Sarah Davison    0.355031\n",
      "Tan Ee           0.354134\n",
      "Todd Nissen      0.351394\n",
      "David Lawder     0.349255\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# finding components in light of AUTHOR\n",
    "components_train = pd.DataFrame(X_train_lsa,index=df_train['Author'])\n",
    "components_test = pd.DataFrame(X_test_lsa,index=df_test['Author'])\n",
    "\n",
    "# five example components:\n",
    "for i in range(5):\n",
    "    print('\\nComponent {} for Training Set:\\n'.format(i))\n",
    "    print(components_train.loc[:,i].sort_values(ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis:__ Here we can see which authors are most similar to each other in light of the 580 (rather abstract) components. \n",
    "\n",
    "We can safely conclude that _at least in some aspects_, no one quite writes like David Lawder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Component 0 for Training Set:\n",
      "\n",
      "Author\n",
      "Kevin Drawbaugh    0.580515\n",
      "Nick Louth         0.563699\n",
      "Eric Auchard       0.556404\n",
      "Graham Earnshaw    0.554592\n",
      "Tim Farrand        0.553483\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1 for Training Set:\n",
      "\n",
      "Author\n",
      "Peter Humphrey    0.761377\n",
      "Peter Humphrey    0.734077\n",
      "Peter Humphrey    0.723136\n",
      "Peter Humphrey    0.704783\n",
      "Benjamin Kang     0.693950\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2 for Training Set:\n",
      "\n",
      "Author\n",
      "Heather Scoffield    0.467673\n",
      "Heather Scoffield    0.467673\n",
      "Heather Scoffield    0.457429\n",
      "Darren Schuettler    0.456426\n",
      "Darren Schuettler    0.448331\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3 for Training Set:\n",
      "\n",
      "Author\n",
      "Darren Schuettler    0.650693\n",
      "Darren Schuettler    0.648150\n",
      "Lydia Zajc           0.595493\n",
      "Darren Schuettler    0.592159\n",
      "Lydia Zajc           0.591638\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4 for Training Set:\n",
      "\n",
      "Author\n",
      "William Kazer      0.424682\n",
      "Graham Earnshaw    0.414584\n",
      "Graham Earnshaw    0.410696\n",
      "Graham Earnshaw    0.408224\n",
      "Graham Earnshaw    0.395817\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('\\nComponent {} for Training Set:\\n'.format(i))\n",
    "    print(components_test.loc[:,i].sort_values(ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis:__ Components appear to be much more consistent in the training set. This makes sense, as we know that LSA explains 20% more variance in the testing dataset than in the training dataset.  This is likely due to the __curse of dimensionality.__ Flattening thousands of vectors down to 580 components will result in crowding and overlapping of data, thus making it more difficult to discern text from Author A from Author B.\n",
    "\n",
    "Because the testing feature space (n=5825) was originally a little over half of that of the training feature space (n=10854), it can work with 580 components more easily.  This is a bias to be taken into account when moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Unsupervised Learning - Clustering Analyses\n",
    "\n",
    "Clustering analysis is meant to find which features are most similar to each other (and in the same vein, which ones are most dissimilar to each other). We can analyze which features fall into which clusters via a `pd.crosstab` analysis.\n",
    "\n",
    "## K-Means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Author</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>72</td>\n",
       "      <td>53</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>11</td>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Author         0   1   2   3   4   5   6   7   8   9  ...  40  41  42  43  44  \\\n",
       "Cluster Label                                         ...                       \n",
       "0              74  41  42   7  35  35  50  26  41  43 ...  72  53  19   7  34   \n",
       "1               0   0   0   0   0   0   0   0   0   0 ...   0   0  40   5   0   \n",
       "2               1  34  33   1  40  40  25   5  34  32 ...   0  22   9   0  41   \n",
       "3               0   0   0  67   0   0   0   0   0   0 ...   3   0   7  63   0   \n",
       "4               0   0   0   0   0   0   0  44   0   0 ...   0   0   0   0   0   \n",
       "\n",
       "Author         45  46  47  48  49  \n",
       "Cluster Label                      \n",
       "0               6  54  11  46  12  \n",
       "1              59   0   0   0   3  \n",
       "2               0  21  64   6   4  \n",
       "3              10   0   0   0  56  \n",
       "4               0   0   0  23   0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_train['AuthorNum']\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "scaler = StandardScaler()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "X_pca = PCA(2).fit_transform(tfidf_vectorized_df_train) # gigantic feature space\n",
    "\n",
    "# PCA spreads data out (as a result of lower dimensionality, don't ask)\n",
    "# Normalizing/standardscaling is just needed for this - otherwise, it will throw a ValueError otherwise.\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans).fit(X_pca)\n",
    "\n",
    "# Calculate the cluster labels: THIS IS THE OUTPUT YOU ARE LOOKING AT!!!!!!!!!!!!********\n",
    "labels = pipeline.predict(X_pca)\n",
    "\n",
    "# Create a DataFrame with cluster labels and Classes as columns: df\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)\n",
    "plt.colorbar() # allows for identification of which color pertains to which cluster number\n",
    "\n",
    "#plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the crosstab data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING ANALYSIS\n",
    "\n",
    "Looking at which vectorized features are the biggest influencers.\n",
    "\n",
    "Here, we will be using *x\\_train\\_tfidf\\_vectorized* as the input data. Tfidf is a measure of frequency, which means the proportions of the raw counts (found in CountVectorized()) are the same among datapoints, but the frequency character of the dataset naturally allows scaling that accounts for any particular outliers, which would otherwise negatively affect clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = df_train['AuthorNum']\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_pca = PCA(2).fit_transform(df_train_tfidf_vectorized) # gigantic feature space\n",
    "\n",
    "# PCA spreads data out (as a result of lower dimensionality, don't ask)\n",
    "# Normalizing/standardscaling is just needed for this - otherwise, it will throw a ValueError otherwise.\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans).fit(X_pca)\n",
    "\n",
    "# Calculate the cluster labels: THIS IS THE OUTPUT YOU ARE LOOKING AT!!!!!!!!!!!!********\n",
    "labels = pipeline.predict(X_pca)\n",
    "\n",
    "# Create a DataFrame with cluster labels and Classes as columns: df\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x')\n",
    "\n",
    "## Calculate predicted values.\n",
    "#y_pred = KMeans(n_clusters=5, random_state=42).fit_predict(X_pca)\n",
    "\n",
    "# Plot the solution.\n",
    "#plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the crosstab data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem with above is that while cluster points scaled and transformed\n",
    "# cluster centers refused to do so\n",
    "\n",
    "# don't need scaling or normalization because all tfidf values are between 0 and 1 anyway\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "# K-Means, Round 2\n",
    "\n",
    "X_pca = PCA(2).fit_transform(df_train_tfidf_vectorized) \n",
    "\n",
    "kmeans = KMeans(n_clusters=13, n_init=50, max_iter=500000, tol=0.000001)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "#from sklearn.preprocessing import Normalizer\n",
    "#normalizer=Normalizer()\n",
    "\n",
    "#pipeline = make_pipeline(normalizer, kmeans).fit_transform(X_pca)\n",
    "kmeans.fit(X_pca)\n",
    "labels = kmeans.predict(X_pca) # same as kmeans.labels_ --- I just did it this way\n",
    "\n",
    "\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "xcenters = cluster_centers[:,0]\n",
    "ycenters = cluster_centers[:,1]\n",
    "xcenlist = list(xcenters)\n",
    "ycenlist = list(ycenters)\n",
    "\n",
    "legendlabels = range(0,13)\n",
    "\n",
    "centerdf = pd.DataFrame({'Xcenter':xcenters, 'Ycenter':ycenters,\n",
    "                        'Labels':[str(label) for label in legendlabels]})\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(x=X_pca[:, 0], y=X_pca[:, 1], c=labels)\n",
    "plt.legend(centerdf.Labels)\n",
    "\n",
    "# Plotting kMeans centers\n",
    "plt.scatter(x=xcenters, y=ycenters,\n",
    "            c='red', marker='x')\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, len(xcenlist)):\n",
    "    print('Cluster {} has center coordinates of: ({}, {})'.format(str(legendlabels[i]), \n",
    "                                                                  round(xcenlist[i], 2), \n",
    "                                                                  round(ycenlist[i], 2)))\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__K-Means Analysis:__\n",
    "\n",
    "The space in the tfidf is very compact, making separation highly difficult. This is expected when you try to reduce a high-dimensional space via PCA; flattening down thousands of dimensions down to `PCA(2)` overcrowds the data, exemplifying the __curse of dimensionality.__  Originally, a large number of clusters (here, K=13) was hypothesized to provide accurate classification information without overfitting and overcome this curse of dimensionality. However, as the crosstab results show, some of the authors' classifications are spread more or less evenly across multiple clusters. This means the model was unable to use the PCA-ed features to classify those particular authors effectively.\n",
    "\n",
    "So how can we maximize the accuracy of the model? What _is_ the accuracy of a clustering model? This is where an _Elbow Visualizer_ and the _Silhouette score_ come in, respectively.\n",
    "\n",
    "A silhouette score for the above model is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing silhouette score\n",
    "\n",
    "from sklearn.metrics import silhouette_score \n",
    "silhouette_score(X_pca, labels) # silhouette score for k=13 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this Silhouette Score Mean?\n",
    "\n",
    "A Silhouette value is a distance measure of a sample's distance to its cluster's center (a) and its distance to the center of the nearest cluster that the sameple is not a part of (b). A __Silhouette Coefficient__ is the average of all of these values for a given cluster. [As used in sklearn.metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), a __Silhouette Score__ is the average of all Silhouette Coefficients across all clusters.\n",
    "\n",
    "Silhouette Coefficients (and by nature of the metric, Silhouette Scores too) range from (-1,1), with \"1\" being the best score. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
    "\n",
    "In short, a silhouette score is a __measure of how accurate the clustering is.__\n",
    "\n",
    "---\n",
    "\n",
    "__Analysis:__ As you can see, our Silhouette Score for k=13 clusters of 36.5% is not particularly comforting. How do we determine the best number of clusters to obtain the highest accuracy for our dataset?  This is where the __K-Elbow Visualizer__ comes in.\n",
    "\n",
    "The \"Elbow Method\" is a simple iterative method that measures the accuracies for a range of k-sized clustering.  The point of inflection (the \"elbow\" of the graph) is usually the indicator that the model fits best with that point (in this case, that the model fits best with that particular k-number of clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not a good percentage! \n",
    "\n",
    "# Import the KElbowVisualizer method \n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate a scikit-learn K-Means model \n",
    "# really cranking on these iterations, \n",
    "# because I have yet to see any overfitting or significant computational overload...\n",
    "model = KMeans(n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric \n",
    "visualizer = KElbowVisualizer(model, k=(2,21), metric='silhouette', timings=True)\n",
    "\n",
    "# Fit the data and visualize \n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "visualizer.fit(X_pca) \n",
    "visualizer.poof() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# judging by the graph, silhouette score for K=2 is ~58%.\n",
    "# welp, that settles it! going back and doing a final K-Means with K=2\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "# K-Means, Round 3\n",
    "\n",
    "X_pca = PCA(2).fit_transform(df_train_tfidf_vectorized) \n",
    "\n",
    "kmeans = KMeans(n_clusters=2, n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "kmeans.fit(X_pca)\n",
    "labels = kmeans.predict(X_pca)\n",
    "\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "xcenters = cluster_centers[:,0]\n",
    "ycenters = cluster_centers[:,1]\n",
    "xcenlist = list(xcenters)\n",
    "ycenlist = list(ycenters)\n",
    "\n",
    "legendlabels = range(0,2)\n",
    "\n",
    "centerdf = pd.DataFrame({'Xcenter':xcenters, 'Ycenter':ycenters,\n",
    "                        'Labels':[str(label) for label in legendlabels]})\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "#matplotlib thought \"black and white\" were good plotting colors for no reason, so that's neat...\n",
    "\n",
    "color_list = []\n",
    "for value in labels:\n",
    "    if value == 0:\n",
    "        color_list.append('orange')\n",
    "    else:\n",
    "        color_list.append('blue')\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(x=X_pca[:, 0], y=X_pca[:, 1], c=color_list) \n",
    "\n",
    "# Plotting kMeans centers\n",
    "plt.scatter(x=xcenters, y=ycenters,\n",
    "            c='red', marker='x')\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, len(xcenlist)):\n",
    "    print('Cluster {} has center coordinates of: ({}, {})'.format(str(legendlabels[i]), \n",
    "                                                                  round(xcenlist[i], 2), \n",
    "                                                                  round(ycenlist[i], 2)))\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_crosstab_analysis = pd.DataFrame({'Author':df_train['Author'].unique()})\n",
    "\n",
    "cluster_fraction_list = []\n",
    "cluster_final_list = []\n",
    "\n",
    "for i in range(0,50):\n",
    "    cluster0_count = ct[i].iloc[0]\n",
    "    cluster1_count = ct[i].iloc[1]\n",
    "    cluster_total = 75\n",
    "    \n",
    "    if cluster0_count > cluster1_count:\n",
    "        cluster0avg = cluster0_count / cluster_total\n",
    "        cluster_fraction_list.append(cluster0avg)\n",
    "        cluster_final_list.append(0)\n",
    "        \n",
    "    else:\n",
    "        cluster1avg = cluster1_count / cluster_total\n",
    "        cluster_fraction_list.append(cluster1avg)\n",
    "        cluster_final_list.append(1)\n",
    "        \n",
    "\n",
    "df_crosstab_analysis['Cluster No. Classification'] = pd.Series(cluster_final_list)\n",
    "df_crosstab_analysis['Percent of Author in the Cluster Classification'] = pd.Series(cluster_fraction_list)\n",
    "df_crosstab_analysis = df_crosstab_analysis.sort_values(by=['Cluster No. Classification',\n",
    "                                                           'Percent of Author in the Cluster Classification'],\n",
    "                                                        ascending=False)\n",
    "\n",
    "# just did this manually\n",
    "print('The percentage of Authors found to be in Cluster 1 more than in Cluster 0 is 20% (10/50 total Authors).')\n",
    "\n",
    "df_crosstab_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis:__ \n",
    "\n",
    "Again, only 20% of the `df_train` dataset can be characterized as falling into Cluster 1. More so, out of that 20%, only two authors (37 - Peter Humphrey and 45 - Tan Ee) were classified in Cluster 1 by more than 75%. These results come from K=2, which has the highest silhouette score of ~58%..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# So what now?\n",
    "\n",
    "Obtaining the highest silhouette score of ~58% leaves us at a point where there is an expected upper-limit to model accuracy moving forward. Did we do something wrong?\n",
    "\n",
    "I propose yes. Yes we did.\n",
    "\n",
    "Thinkful's Capstone Project instructions appear to place the cart before the horse. According to the instructions (which are listed in the beginning of this project): \"The _first technique_ is to create a series of clusters. ... _Next_, perform some unsupervised feature generation and selection using techniques covered in this unit and elsewhere in the course.\"\n",
    "\n",
    "It is a fact that dimensionality reduction and proper scaling are _required_ for clustering. While not synonymous, dimensionality reduction goes hand in hand with feature selection and generation; the dimensionality reduction algorithm is created \"new\" composite features as projections. From there, we can cluster and perform unsupervised learning.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Table of Contents for Remainder of Project\n",
    "\n",
    "1. Re-vectorize data using tfidf again, but with different (better) hyperparameters.\n",
    "\n",
    "\n",
    "2. Use gensim's `word2vec` to perform unsupervised learning on the dataset (this model has dimensionality reduction built-in as a hyperparameter).\n",
    "\n",
    "\n",
    "3. Reduce dimensionality using the more appropriate dimensionality reduction algorithm, _Latent Semantic Analysis (LSA)_, for the remainder of unsupervised learning analysis.\n",
    "    - retain original `df_train` for aggregate number features. This will be separately clustered and analyzed.\n",
    "    \n",
    "    \n",
    "4. Apply clustering models:\n",
    "    - K-Means (again)\n",
    "    - Spectral Clustering\n",
    "    - t-SNE\n",
    "\n",
    "\n",
    "5. Perform any feature selection, testing whether to include aggregate number features in the vectorized DataFrame.\n",
    "\n",
    "\n",
    "6. Build and fit supervised classification models.\n",
    "\n",
    "\n",
    "7. Test fitted models on vectorized and dimensionality-reduced testing data.\n",
    "\n",
    "\n",
    "8. Apply test data to previously used clustering models.\n",
    "    - Compare and contrast the results with those of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: Load the data\n",
    "# I have since restarted the kernel\n",
    "# luckily, I saved my text analysis df's\n",
    "\n",
    "df_train = pd.read_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/COMPLETE_NLP-train.csv')\n",
    "df_test = pd.read_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/COMPLETE_NLP-test.csv')\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not so lucky, it appears that python doesn't know what to do with lemmatized text as that is a spacy datatype\n",
    "# gotta re-clean?...\n",
    "\n",
    "# STEP 1: DATA CLEANING\n",
    "# wanted to do something more comprehensive than the above text cleaner function\n",
    "\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('\\'', ''))\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('[', ''))\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace(']', ''))\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace(',', ''))\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace(r'\\\\n', ''))\n",
    "df_train['Lemmatized Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('.', '')) # tried to do re\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('-', ' '))# but didn't work...\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('\\\\n', ''))\n",
    "df_train['Lemmatized Text'][1]\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's...fine.\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('\\'', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('[', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace(']', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace(',', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('.', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('-', ' '))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('\\\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - RE-VECTORIZING TRAINING DATA\n",
    "\n",
    "# Instantiate vectorizer model\n",
    "# input here for demonstration purposes - already imported\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word',\n",
    "                        ngram_range=(1,1),      # ensuring unigram nature\n",
    "                        min_df=2,               # only use words that appear at least twice******\n",
    "                        stop_words='english',   # stop words and lowercase were already done \n",
    "                        lowercase=True,         # but it doesn't hurt to do a second time\n",
    "                        use_idf=True,           # um yeah, that's what we're doing...\n",
    "                        norm=u'l2',             # Applies a correction factor for imbalanced sized tokens\n",
    "                        smooth_idf=True         # smooth_idf adds 1 to all document frequencies, \n",
    "                       )                        # as if an extra document existed that used every word once.  \n",
    "                                                # Prevents divide-by-zero errors\n",
    "    \n",
    "tfidf_vectors = tfidf.fit_transform(df_train['Lemmatized Text'])\n",
    "#dense = tfidf_vectors.todense() ----- for a feature space of this size\n",
    "#denselist = dense.tolist()     ------ fixing sparsity will crash/run out of memory\n",
    "\n",
    "token_names = tfidf.get_feature_names()\n",
    "\n",
    "# creating dataframe from vectors (a separate process)\n",
    "vectorized_df_train = pd.DataFrame(tfidf_vectors.toarray(), columns=token_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAFRAME CLEANING\n",
    "'''\n",
    "This vectorized dataframe is extremely sparse.\n",
    "Trying to do tfidf_vectors.todense() threw a memory error. Not risking that.\n",
    "Dropping all number columns because we are looking at words, not numbers.\n",
    "'''\n",
    "import re\n",
    "\n",
    "numbertokens = []\n",
    "dates = [str(x) for x in list(range(1900, 2026))]\n",
    "\n",
    "for name in token_names:\n",
    "    num_token = re.findall(r'\\d+[^t][^h]', name)\n",
    "    num_not_date = [x for x in num_token if x not in dates]\n",
    "    if len(num_not_date) != 0:\n",
    "        numbertokens.append(num_not_date)\n",
    "        \n",
    "print(len(numbertokens))\n",
    "numbertokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbertokens is list of lists. Need to undo that\n",
    "from itertools import chain\n",
    "\n",
    "numbertokens2 = list(chain.from_iterable(numbertokens))\n",
    "numbertokens2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbertokens2[25:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(numbertokens2))\n",
    "print(len(list(set(numbertokens2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates, there are none\n",
    "print(len(token_names))\n",
    "print(len(set(token_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING to drop all numerical tokens\n",
    "\n",
    "for item in numbertokens2 and token_names:\n",
    "    vectorized_df_train.drop(columns=[item], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I give up, just manually doing it:\n",
    "vectorized_df_train = vectorized_df_train.iloc[:,1808:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis and Dimensionality Reduction via LSA\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a dimensionality reduction method that measures the angles of the vectors via a cosine algorithm. This cosine algorithm essentially creates a normal range between 0 and 1.\n",
    "\n",
    "For efficiency purposes, dimensionality reduction is crucial for unsupervised learning methods such as clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from nearly 20000 to 150\n",
    "svd= TruncatedSVD(150)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(vectorized_df_train) ###### LSA IS THE NAME OF THE MODEL USED HERE!\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of authors our LSA model considers similar, for the first (not top) ***FIVE*** components\n",
    "components=pd.DataFrame(X_train_lsa,index=df_train['Author'])\n",
    "for i in range(5):\n",
    "    print('\\nComponent {}:\\n'.format(i))\n",
    "    print(components.loc[:,i].sort_values(ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this tell us?\n",
    "\n",
    "Tells us that no one writes like David Lawder!\n",
    "\n",
    "It is disheartening, however, to see that the truncation resulted in the retention of only 37% variance. While this is somewhat expected given that we are flattening nearly 20000 features into 150, this is still a significant loss and could play a role in making our future models inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if LSA can tell us \n",
    "\n",
    "# running LSA again with 50 components, like 50 clusters:\n",
    "svd= TruncatedSVD(50)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(vectorized_df_train) ###### LSA IS THE NAME OF THE MODEL USED HERE!\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "components=pd.DataFrame(X_train_lsa,index=df_train['Author'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsort = {}\n",
    "for column in components.columns:\n",
    "    columndict = {str(column):components[column].max()}\n",
    "    columnsort.update(columndict)\n",
    "    \n",
    "columnsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components 0,1,3,7 had highest numbers:\n",
    "\n",
    "for i in [0,1,3,7]:\n",
    "    print('\\nComponent {}:\\n'.format(i))\n",
    "    print(components.loc[:,i].sort_values(ascending=False)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above to figure out which components best describe a given author.\n",
    "\n",
    "To do this, we could use `pd.idmax(axis=0)`\n",
    "\n",
    "# Clustering\n",
    "\n",
    "Using the above, we have an idea of what kinds of clusterings we expect from authors' writings. Unforntuately, we have to use reduce dimensionality down to 2 if we want to visualize the clusters. SVD is still preferable over PCA because PCA requires a central tendency, and there is no such thing in a sparse vector matrix. On the other hand, PCA retains variance, and we have seen the huge drops in variance explainability with SVD/LSA.\n",
    "\n",
    "Let's try SVD first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-doing SVD to get 2 dimensions\n",
    "svd= TruncatedSVD(2)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(vectorized_df_train)\n",
    "\n",
    "# K-Means clustering\n",
    "y = df_train['Author']\n",
    "kmeans = KMeans(n_clusters=5, n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "kmeans.fit(X_train_lsa)\n",
    "labels = kmeans.predict(X_train_lsa)\n",
    "\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "xcenters = cluster_centers[:,0]\n",
    "ycenters = cluster_centers[:,1]\n",
    "xcenlist = list(xcenters)\n",
    "ycenlist = list(ycenters)\n",
    "\n",
    "centerdf = pd.DataFrame({'Xcenter':xcenters, 'Ycenter':ycenters})\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(x=X_train_lsa[:, 0], y=X_train_lsa[:, 1], c=labels) \n",
    "\n",
    "# Plotting kMeans centers\n",
    "plt.scatter(x=xcenters, y=ycenters,\n",
    "            c='red', marker='x')\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, len(xcenlist)):\n",
    "    print('Cluster {} has center coordinates of: ({}, {})'.format(i, \n",
    "                                                                  round(xcenlist[i], 2), \n",
    "                                                                  round(ycenlist[i], 2)))\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, creating a central tendency for graphing purposes might be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_pca = PCA(2).fit_transform(vectorized_df_train) \n",
    "\n",
    "kmeans = KMeans(n_clusters=5, n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans).fit(X_pca)\n",
    "\n",
    "# Calculate the cluster labels\n",
    "labels = pipeline.predict(X_pca)\n",
    "\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "xcenters = cluster_centers[:,0]\n",
    "ycenters = cluster_centers[:,1]\n",
    "xcenlist = list(xcenters)\n",
    "ycenlist = list(ycenters)\n",
    "\n",
    "centerdf = pd.DataFrame({'Xcenter':xcenters, 'Ycenter':ycenters})\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(x=X_pca[:, 0], y=X_pca[:, 1], c=labels) \n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(5):\n",
    "    print('Cluster {} has center coordinates of: ({}, {})'.format(i, \n",
    "                                                                  round(xcenlist[i], 2), \n",
    "                                                                  round(ycenlist[i], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing silhouette score\n",
    "\n",
    "from sklearn.metrics import silhouette_score \n",
    "silhouette_score(X_pca, labels) # silhouette score for k=5 clusters, above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing one more KElbow analysis\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate a scikit-learn K-Means model \n",
    "# really cranking on these iterations, \n",
    "# because I have yet to see any overfitting or significant computational overload...\n",
    "model = KMeans(n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric \n",
    "visualizer = KElbowVisualizer(model, k=(2,21), metric='silhouette', timings=True)\n",
    "\n",
    "# Fit the data and visualize \n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "visualizer.fit(X_pca) \n",
    "visualizer.poof() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do\n",
    "\n",
    "run mean shift with PCA\n",
    "run spectral OR affinity with PCA\n",
    "run t-SNE\n",
    "---\n",
    "create feature spaces, incorporating top components from LSA and aggregate numbers from original\n",
    "DO THE SAME FOR XTEST\n",
    "run supervised classification models\n",
    "- lasso/ridge\n",
    "- knn\n",
    "- tree model/boosting model\n",
    "unsupervised modelling on testing dataset\n",
    "explain classification analyis and compare/contrast models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SciPy hierarchical clustering doesn't fit into a sklearn pipeline, so we need to use the \n",
    "# normalize() function from sklearn.preprocessing instead of Normalizer.\n",
    "\n",
    "# Import normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "# Normalize the movements: normalized_movements\n",
    "normalized_movements = normalize(sample_data)\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(normalized_movements, method='complete')\n",
    "\n",
    "\n",
    "# Plot the dendrogram, using varieties as labels\n",
    "dendrogram(mergings,labels=varieties, leaf_rotation=90,leaf_font_size=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing around with the K ---- checking elbow for best number of clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
