{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 Capstone - News Article Analysis & Classification\n",
    "\n",
    "## John A. Fonte\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Find 100 different entries from at least 10 different authors (articles?)\n",
    "2. Reserve 25% for test set\n",
    "3. cluster vectorized data (go through a few clustering methods)\n",
    "4. Perform unsupervised feature generation and selection\n",
    "5. Perform supervised modeling by classifying by author\n",
    "6. Comment on your 25% holdout group. Did the clusters for the holdout group change dramatically, or were they consistent with the training groups? Is the performance of the model consistent? If not, why?\n",
    "7. Conclude with which models (clustering or not) work best for classifying texts.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "__Source:__ https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#\n",
    "\n",
    "__Description:__ This is a subset of the [Reuters Corpus Volume 1 (RCV1)](https://scikit-learn.org/0.17/datasets/rcv1.html). Specifically, this subset consists of the top 50 authors by article proliferation, with a total of 100 articles per each author within the combined training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 1. Data Load and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Data from Local Computer\n",
    "Each author is a subfolder, and within each folder is a series of .txt files\n",
    "The goal of this cell is to load all the contents of every subfolder into the \n",
    "DataFrame, while retaining the author designation for those works.\n",
    "'''\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "def multiple_file_load(file_directory):\n",
    "    \n",
    "    # identifying all author subfolders - appending them into list \n",
    "    \n",
    "    authorlist = []\n",
    "    textlist = []\n",
    "    \n",
    "    for author in listdir(file_directory):\n",
    "        authorname = str(author)\n",
    "        author_sub_directory = (file_directory + '/' + author) #author file path\n",
    "    \n",
    "    # identifying all files within each subfolder - \n",
    "    \n",
    "        for filename in listdir(author_sub_directory):\n",
    "            text_file_path = (author_sub_directory + '/' + filename) # text file path\n",
    "            \n",
    "            if (filename.lower().endswith('txt')):\n",
    "                authorlist.append(authorname)\n",
    "                textfile = open(text_file_path,'r') # this is how you open files\n",
    "                substantive_text = textfile.read()  # this is how to read a file\n",
    "                textlist.append(substantive_text)   # this is how to do something with that file\n",
    "                textfile.close()                    # this is how to close the file \n",
    "                                                             # (you must close one before opening another!)\n",
    "  # pushing the two lists into a dataframe \n",
    "\n",
    "    df = pd.DataFrame({'Author':authorlist, 'Text':textlist})\n",
    "    \n",
    "    return df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data (note the file path)\n",
    "df_train = multiple_file_load('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/C50/C50train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author                                               Text\n",
       "0  AaronPressman  The Internet may be overflowing with new techn...\n",
       "1  AaronPressman  The U.S. Postal Service announced Wednesday a ...\n",
       "2  AaronPressman  Elementary school students with access to the ...\n",
       "3  AaronPressman  An influential Internet organisation has backe...\n",
       "4  AaronPressman  An influential Internet organisation has backe..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the space in the authors...because I want it\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_split = [re.findall('[A-Z][a-z]*', i) for i in df_train.Author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Aaron', 'Pressman'],\n",
       " ['Aaron', 'Pressman'],\n",
       " ['Aaron', 'Pressman'],\n",
       " ['Aaron', 'Pressman'],\n",
       " ['Aaron', 'Pressman']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_split[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining them back together\n",
    "author_join = []\n",
    "\n",
    "for couple in author_split:\n",
    "    joined_string = couple[0] + ' ' + couple[1]\n",
    "    author_join.append(joined_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China's central bank chief has said that infla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China ushered in 1997, a year it has hailed as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China issued tough new rules on the handling o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China will avoid bold moves in tackling its ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>Communist Party chief Jiang Zemin has put his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author                                               Text\n",
       "2495  William Kazer  China's central bank chief has said that infla...\n",
       "2496  William Kazer  China ushered in 1997, a year it has hailed as...\n",
       "2497  William Kazer  China issued tough new rules on the handling o...\n",
       "2498  William Kazer  China will avoid bold moves in tackling its ai...\n",
       "2499  William Kazer  Communist Party chief Jiang Zemin has put his ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Author'] = pd.Series(author_join)\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>U.S. Senators on Tuesday sharply criticized a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>Two members of Congress criticised the Federal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>Commuters stuck in traffic on the Leesburg Pik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>A broad coalition of corporations went to Capi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>On the Internet, where new products come and g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author                                               Text\n",
       "0  AaronPressman  U.S. Senators on Tuesday sharply criticized a ...\n",
       "1  AaronPressman  Two members of Congress criticised the Federal...\n",
       "2  AaronPressman  Commuters stuck in traffic on the Leesburg Pik...\n",
       "3  AaronPressman  A broad coalition of corporations went to Capi...\n",
       "4  AaronPressman  On the Internet, where new products come and g..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before I begin adding features, assignment asks for 25% data split, NOT 50/50\n",
    "# Going to have to concat some of those testing articles\n",
    "df_test = multiple_file_load('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/C50/C50test')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another fix to Author column\n",
    "\n",
    "author_split = [re.findall('[A-Z][a-z]*', i) for i in df_test.Author]\n",
    "\n",
    "author_join = []\n",
    "\n",
    "for couple in author_split:\n",
    "    joined_string = couple[0] + ' ' + couple[1]\n",
    "    author_join.append(joined_string)    \n",
    "    \n",
    "df_test['Author'] = pd.Series(author_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GOAL:\n",
    "Trying to get half of the datapoints OF EACH AUTHOR\n",
    "in the testing set into a new DataFrame, which\n",
    "will be concatenated onto the training set.\n",
    "I will delete that from the testing set later.\n",
    "\n",
    "Doing this instead of combining both and splitting 75/25 later \n",
    "ensures balanced data between the authors.\n",
    "'''\n",
    "\n",
    "def appendingdataframe(dataframe):\n",
    "    appendabledataframe = pd.DataFrame(columns=['Author', 'Text'])\n",
    "    \n",
    "    for item in dataframe.Author.unique():\n",
    "        print(\"Looping through \", item)\n",
    "        df_testauthor = df_test[df_test['Author'] == item].copy() \n",
    "        appendabledataframe = appendabledataframe.append(df_testauthor[25:], \n",
    "                                                         ignore_index=True) # want half of df_testauthor!\n",
    "    \n",
    "    return appendabledataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through  Aaron Pressman\n",
      "Looping through  Alan Crosby\n",
      "Looping through  Alexander Smith\n",
      "Looping through  Benjamin Kang\n",
      "Looping through  Bernard Hickey\n",
      "Looping through  Brad Dorfman\n",
      "Looping through  Darren Schuettler\n",
      "Looping through  David Lawder\n",
      "Looping through  Edna Fernandes\n",
      "Looping through  Eric Auchard\n",
      "Looping through  Fumiko Fujisaki\n",
      "Looping through  Graham Earnshaw\n",
      "Looping through  Heather Scoffield\n",
      "Looping through  Jane Macartney\n",
      "Looping through  Jan Lopatka\n",
      "Looping through  Jim Gilchrist\n",
      "Looping through  Joe Ortiz\n",
      "Looping through  John Mastrini\n",
      "Looping through  Jonathan Birt\n",
      "Looping through  Jo Winterbottom\n",
      "Looping through  Karl Penhaul\n",
      "Looping through  Keith Weir\n",
      "Looping through  Kevin Drawbaugh\n",
      "Looping through  Kevin Morrison\n",
      "Looping through  Kirstin Ridley\n",
      "Looping through  Kourosh Karimkhany\n",
      "Looping through  Lydia Zajc\n",
      "Looping through  Lynne O\n",
      "Looping through  Lynnley Browning\n",
      "Looping through  Marcel Michelson\n",
      "Looping through  Mark Bendeich\n",
      "Looping through  Martin Wolk\n",
      "Looping through  Matthew Bunce\n",
      "Looping through  Michael Connor\n",
      "Looping through  Mure Dickie\n",
      "Looping through  Nick Louth\n",
      "Looping through  Patricia Commins\n",
      "Looping through  Peter Humphrey\n",
      "Looping through  Pierre Tran\n",
      "Looping through  Robin Sidel\n",
      "Looping through  Roger Fillion\n",
      "Looping through  Samuel Perry\n",
      "Looping through  Sarah Davison\n",
      "Looping through  Scott Hillis\n",
      "Looping through  Simon Cowell\n",
      "Looping through  Tan Ee\n",
      "Looping through  Therese Poletti\n",
      "Looping through  Tim Farrand\n",
      "Looping through  Todd Nissen\n",
      "Looping through  William Kazer\n"
     ]
    }
   ],
   "source": [
    "# using appendabledataframe to avoid screwing up original data\n",
    "# This is explicit inefficiency at the cost of being cautious\n",
    "\n",
    "df_train2 = df_train.append(appendingdataframe(df_train), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if the appending worked\n",
    "len(df_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through  Aaron Pressman\n",
      "Looping through  Alan Crosby\n",
      "Looping through  Alexander Smith\n",
      "Looping through  Benjamin Kang\n",
      "Looping through  Bernard Hickey\n",
      "Looping through  Brad Dorfman\n",
      "Looping through  Darren Schuettler\n",
      "Looping through  David Lawder\n",
      "Looping through  Edna Fernandes\n",
      "Looping through  Eric Auchard\n",
      "Looping through  Fumiko Fujisaki\n",
      "Looping through  Graham Earnshaw\n",
      "Looping through  Heather Scoffield\n",
      "Looping through  Jane Macartney\n",
      "Looping through  Jan Lopatka\n",
      "Looping through  Jim Gilchrist\n",
      "Looping through  Joe Ortiz\n",
      "Looping through  John Mastrini\n",
      "Looping through  Jonathan Birt\n",
      "Looping through  Jo Winterbottom\n",
      "Looping through  Karl Penhaul\n",
      "Looping through  Keith Weir\n",
      "Looping through  Kevin Drawbaugh\n",
      "Looping through  Kevin Morrison\n",
      "Looping through  Kirstin Ridley\n",
      "Looping through  Kourosh Karimkhany\n",
      "Looping through  Lydia Zajc\n",
      "Looping through  Lynne O\n",
      "Looping through  Lynnley Browning\n",
      "Looping through  Marcel Michelson\n",
      "Looping through  Mark Bendeich\n",
      "Looping through  Martin Wolk\n",
      "Looping through  Matthew Bunce\n",
      "Looping through  Michael Connor\n",
      "Looping through  Mure Dickie\n",
      "Looping through  Nick Louth\n",
      "Looping through  Patricia Commins\n",
      "Looping through  Peter Humphrey\n",
      "Looping through  Pierre Tran\n",
      "Looping through  Robin Sidel\n",
      "Looping through  Roger Fillion\n",
      "Looping through  Samuel Perry\n",
      "Looping through  Sarah Davison\n",
      "Looping through  Scott Hillis\n",
      "Looping through  Simon Cowell\n",
      "Looping through  Tan Ee\n",
      "Looping through  Therese Poletti\n",
      "Looping through  Tim Farrand\n",
      "Looping through  Todd Nissen\n",
      "Looping through  William Kazer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing same for df_test\n",
    "\n",
    "df_test2 = df_test.append(appendingdataframe(df_train), ignore_index=True)\n",
    "\n",
    "# checking if the appending worked\n",
    "len(df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now to drop the rows added to df_train from df_test\n",
    "\n",
    "df_test2.drop_duplicates(keep=False, inplace=True)\n",
    "len(df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE About Further Cleaning\n",
    "\n",
    "Usually, text cleaning requires the deletion of peculiar characters that were scraped alongside the raw text (such as repeated backslashes or brackets around the text). These would need to be cleaned out first before vectorization, as the provided vectorizers could not accommodate those random characters.\n",
    "\n",
    "When looking at the data here, it appears that the raw texts are clean, and so we can thankfully skip that step. We can now jump right into feature engineering:\n",
    "\n",
    "---\n",
    "\n",
    "# Adding Features\n",
    "\n",
    "Just some fun numerical features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some numerical features for text analysis\n",
    "\n",
    "df_train['Raw Character Count'] = df_train['Text'].apply(lambda x: len(x))\n",
    "df_train['Raw Word Count'] = df_train['Text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing same for df_test\n",
    "\n",
    "df_test['Raw Character Count'] = df_test['Text'].apply(lambda x: len(x))\n",
    "df_test['Raw Word Count'] = df_test['Text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating numerical classes for authors:\n",
    "# I feel like one hot encoding would've screwed things up, so I did \"factorize\"\n",
    "\n",
    "df_train['AuthorNum'] = pd.factorize(df_train.Author)[0]\n",
    "df_train['AuthorNum'] = df_train['AuthorNum'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Raw Character Count</th>\n",
       "      <th>Raw Word Count</th>\n",
       "      <th>AuthorNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China has scored new successes in its fight ag...</td>\n",
       "      <td>2473</td>\n",
       "      <td>411</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China has scored new successes in its fight ag...</td>\n",
       "      <td>2473</td>\n",
       "      <td>411</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China is on target with plans to to promote 10...</td>\n",
       "      <td>1742</td>\n",
       "      <td>287</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China may need to adjust the mix of its treasu...</td>\n",
       "      <td>3263</td>\n",
       "      <td>546</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>A Chinese ideologue known for his strictly ort...</td>\n",
       "      <td>3026</td>\n",
       "      <td>483</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author                                               Text  \\\n",
       "3745  William Kazer  China has scored new successes in its fight ag...   \n",
       "3746  William Kazer  China has scored new successes in its fight ag...   \n",
       "3747  William Kazer  China is on target with plans to to promote 10...   \n",
       "3748  William Kazer  China may need to adjust the mix of its treasu...   \n",
       "3749  William Kazer  A Chinese ideologue known for his strictly ort...   \n",
       "\n",
       "      Raw Character Count  Raw Word Count AuthorNum  \n",
       "3745                 2473             411        49  \n",
       "3746                 2473             411        49  \n",
       "3747                 1742             287        49  \n",
       "3748                 3263             546        49  \n",
       "3749                 3026             483        49  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and same for df_test...\n",
    "\n",
    "df_test['AuthorNum'] = pd.factorize(df_test.Author)[0]\n",
    "df_test['AuthorNum'] = df_test['AuthorNum'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Raw Character Count</th>\n",
       "      <th>Raw Word Count</th>\n",
       "      <th>AuthorNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China's Foreign Minister Qian Qichen on Friday...</td>\n",
       "      <td>1827</td>\n",
       "      <td>299</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China blamed criminal elements on Sunday for a...</td>\n",
       "      <td>3156</td>\n",
       "      <td>516</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>An unemployed Taiwanese journalist on Monday d...</td>\n",
       "      <td>3000</td>\n",
       "      <td>492</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China moved ahead on Wednesday with plans to h...</td>\n",
       "      <td>3762</td>\n",
       "      <td>590</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>Premier Li Peng said on Friday China wanted a ...</td>\n",
       "      <td>2417</td>\n",
       "      <td>408</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author                                               Text  \\\n",
       "2470  William Kazer  China's Foreign Minister Qian Qichen on Friday...   \n",
       "2471  William Kazer  China blamed criminal elements on Sunday for a...   \n",
       "2472  William Kazer  An unemployed Taiwanese journalist on Monday d...   \n",
       "2473  William Kazer  China moved ahead on Wednesday with plans to h...   \n",
       "2474  William Kazer  Premier Li Peng said on Friday China wanted a ...   \n",
       "\n",
       "      Raw Character Count  Raw Word Count AuthorNum  \n",
       "2470                 1827             299        49  \n",
       "2471                 3156             516        49  \n",
       "2472                 3000             492        49  \n",
       "2473                 3762             590        49  \n",
       "2474                 2417             408        49  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the spacyed text\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick cleaning of text:\n",
    "\n",
    "df_train['Text'] = df_train['Text'].apply(lambda x: text_cleaner(x))\n",
    "df_test['Text'] = df_test['Text'].apply(lambda x: text_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a \"Meaningful Word Count\" via spacy implementation\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "df_train['Spacy-ed Text'] = df_train['Text'].apply(lambda text: nlp(text))\n",
    "\n",
    "# Spacy-ing will take a LONG LONG LONG LONNNNNNNNNGGGGGGG TIME TO LOAD. BE PATIENT!\n",
    "# run-time (about five minutes for df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Spacy-ed Text'] = df_test['Text'].apply(lambda text: nlp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up function to (1) lemmatize AND\n",
    "# (2) exclude stop words from the count\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemmas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_) # this is why we needed to spacy/nlp-ify the texts first\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Meaningful Word Count'] = df_train['Spacy-ed Text'].apply(lambda text: lemma_frequencies(text, \n",
    "                                                                                                   include_stop=False))\n",
    "df_test['Meaningful Word Count'] = df_test['Spacy-ed Text'].apply(lambda text: lemma_frequencies(text,\n",
    "                                                                                                include_stop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-48db52186797>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spacy-ed Text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spacy-ed Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spacy-ed Text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spacy-ed Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-48db52186797>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spacy-ed Text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spacy-ed Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spacy-ed Text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spacy-ed Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-b6ff7b797384>\u001b[0m in \u001b[0;36mtext_cleaner\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# cleaning the spacyed text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtext_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'--'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[\\[].*?[\\]]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "df_train['Spacy-ed Text'] = df_train['Spacy-ed Text'].apply(lambda x: text_cleaner(x))\n",
    "df_test['Spacy-ed Text'] = df_test['Spacy-ed Text'].apply(lambda x: text_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Raw Character Count</th>\n",
       "      <th>Raw Word Count</th>\n",
       "      <th>AuthorNum</th>\n",
       "      <th>Spacy-ed Text</th>\n",
       "      <th>Meaningful Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>U.S. Senators on Tuesday sharply criticized a ...</td>\n",
       "      <td>3804</td>\n",
       "      <td>599</td>\n",
       "      <td>0</td>\n",
       "      <td>(U.S., Senators, on, Tuesday, sharply, critici...</td>\n",
       "      <td>{'U.S.': 1, 'senator': 3, 'Tuesday': 3, 'sharp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>Two members of Congress criticised the Federal...</td>\n",
       "      <td>2158</td>\n",
       "      <td>349</td>\n",
       "      <td>0</td>\n",
       "      <td>(Two, members, of, Congress, criticised, the, ...</td>\n",
       "      <td>{'member': 1, 'Congress': 2, 'criticise': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>Commuters stuck in traffic on the Leesburg Pik...</td>\n",
       "      <td>6403</td>\n",
       "      <td>1048</td>\n",
       "      <td>0</td>\n",
       "      <td>(Commuters, stuck, in, traffic, on, the, Leesb...</td>\n",
       "      <td>{'commuter': 1, 'stick': 1, 'traffic': 10, 'Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>A broad coalition of corporations went to Capi...</td>\n",
       "      <td>2223</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "      <td>(A, broad, coalition, of, corporations, went, ...</td>\n",
       "      <td>{'broad': 1, 'coalition': 1, 'corporation': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>On the Internet, where new products come and g...</td>\n",
       "      <td>4227</td>\n",
       "      <td>690</td>\n",
       "      <td>0</td>\n",
       "      <td>(On, the, Internet, ,, where, new, products, c...</td>\n",
       "      <td>{'internet': 10, 'new': 1, 'product': 1, 'come...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author                                               Text  \\\n",
       "0  Aaron Pressman  U.S. Senators on Tuesday sharply criticized a ...   \n",
       "1  Aaron Pressman  Two members of Congress criticised the Federal...   \n",
       "2  Aaron Pressman  Commuters stuck in traffic on the Leesburg Pik...   \n",
       "3  Aaron Pressman  A broad coalition of corporations went to Capi...   \n",
       "4  Aaron Pressman  On the Internet, where new products come and g...   \n",
       "\n",
       "   Raw Character Count  Raw Word Count AuthorNum  \\\n",
       "0                 3804             599         0   \n",
       "1                 2158             349         0   \n",
       "2                 6403            1048         0   \n",
       "3                 2223             329         0   \n",
       "4                 4227             690         0   \n",
       "\n",
       "                                       Spacy-ed Text  \\\n",
       "0  (U.S., Senators, on, Tuesday, sharply, critici...   \n",
       "1  (Two, members, of, Congress, criticised, the, ...   \n",
       "2  (Commuters, stuck, in, traffic, on, the, Leesb...   \n",
       "3  (A, broad, coalition, of, corporations, went, ...   \n",
       "4  (On, the, Internet, ,, where, new, products, c...   \n",
       "\n",
       "                               Meaningful Word Count  \n",
       "0  {'U.S.': 1, 'senator': 3, 'Tuesday': 3, 'sharp...  \n",
       "1  {'member': 1, 'Congress': 2, 'criticise': 1, '...  \n",
       "2  {'commuter': 1, 'stick': 1, 'traffic': 10, 'Le...  \n",
       "3  {'broad': 1, 'coalition': 1, 'corporation': 1,...  \n",
       "4  {'internet': 10, 'new': 1, 'product': 1, 'come...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# didn't want a dictionary. I wanted a total:\n",
    "\n",
    "df_train['Meaningful Word Count Total'] = df_train['Meaningful Word Count'].apply(lambda x: sum(list(x.values())))\n",
    "df_test['Meaningful Word Count Total'] = df_test['Meaningful Word Count'].apply(lambda x: sum(list(x.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>Raw Character Count</th>\n",
       "      <th>Raw Word Count</th>\n",
       "      <th>AuthorNum</th>\n",
       "      <th>Spacy-ed Text</th>\n",
       "      <th>Meaningful Word Count</th>\n",
       "      <th>Meaningful Word Count Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "      <td>2009</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, Internet, may, be, overflowing, with, ne...</td>\n",
       "      <td>{'internet': 8, 'overflow': 1, 'new': 1, 'tech...</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "      <td>2604</td>\n",
       "      <td>416</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, U.S., Postal, Service, announced, Wednes...</td>\n",
       "      <td>{'U.S.': 1, 'Postal': 4, 'Service': 4, 'announ...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "      <td>491</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>(Elementary, school, students, with, access, t...</td>\n",
       "      <td>{'elementary': 1, 'school': 3, 'student': 1, '...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "      <td>2907</td>\n",
       "      <td>463</td>\n",
       "      <td>0</td>\n",
       "      <td>(An, influential, Internet, organisation, has,...</td>\n",
       "      <td>{'influential': 1, 'internet': 5, 'organisatio...</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Pressman</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "      <td>2299</td>\n",
       "      <td>356</td>\n",
       "      <td>0</td>\n",
       "      <td>(An, influential, Internet, organisation, has,...</td>\n",
       "      <td>{'influential': 1, 'internet': 6, 'organisatio...</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author                                               Text  \\\n",
       "0  Aaron Pressman  The Internet may be overflowing with new techn...   \n",
       "1  Aaron Pressman  The U.S. Postal Service announced Wednesday a ...   \n",
       "2  Aaron Pressman  Elementary school students with access to the ...   \n",
       "3  Aaron Pressman  An influential Internet organisation has backe...   \n",
       "4  Aaron Pressman  An influential Internet organisation has backe...   \n",
       "\n",
       "   Raw Character Count  Raw Word Count AuthorNum  \\\n",
       "0                 2009             319         0   \n",
       "1                 2604             416         0   \n",
       "2                  491              72         0   \n",
       "3                 2907             463         0   \n",
       "4                 2299             356         0   \n",
       "\n",
       "                                       Spacy-ed Text  \\\n",
       "0  (The, Internet, may, be, overflowing, with, ne...   \n",
       "1  (The, U.S., Postal, Service, announced, Wednes...   \n",
       "2  (Elementary, school, students, with, access, t...   \n",
       "3  (An, influential, Internet, organisation, has,...   \n",
       "4  (An, influential, Internet, organisation, has,...   \n",
       "\n",
       "                               Meaningful Word Count  \\\n",
       "0  {'internet': 8, 'overflow': 1, 'new': 1, 'tech...   \n",
       "1  {'U.S.': 1, 'Postal': 4, 'Service': 4, 'announ...   \n",
       "2  {'elementary': 1, 'school': 3, 'student': 1, '...   \n",
       "3  {'influential': 1, 'internet': 5, 'organisatio...   \n",
       "4  {'influential': 1, 'internet': 6, 'organisatio...   \n",
       "\n",
       "   Meaningful Word Count Total  \n",
       "0                          201  \n",
       "1                          259  \n",
       "2                           52  \n",
       "3                          265  \n",
       "4                          208  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember when I said I didn't have to clean?\n",
    "# Yeah, well, I have to clean the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing! Changing Text to Numbers\n",
    "\n",
    "Once everything is numerical, then we can feed that data into the clusters for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = df_train.copy()\n",
    "df_test2 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'\\n': 11, 'internet': 8, 'scam': 6, 'site': 6, 'say': 4, 'league': 4, 'consumer': 4, 'Fraud': 4, '$': 4, 'popular': 3, 'investor': 3, 'web': 3, 'Watch': 3, 'report': 3, 'League': 2, 'scheme': 2, 'bogus': 2, 'deposit': 2, 'commission': 2, 'Fortuna': 2, 'promise': 2, 'business': 2, 'overflow': 1, 'new': 1, 'technology': 1, 'crime': 1, 'cyberspace': 1, 'old': 1, 'fashioned': 1, 'variety': 1, 'National': 1, 'Consumers': 1, 'Wednesday': 1, 'pyramid': 1, 'early': 1, 'fund': 1, 'pay': 1, 'later': 1, 'non': 1, 'profit': 1, 'advocacy': 1, 'group': 1, 'track': 1, 'set': 1, 'world': 1, 'wide': 1, 'February': 1, 'call': 1, 'Internet': 1, 'http://www.fraud.org': 1, 'collect': 1, 'directly': 1, 'widely': 1, 'praise': 1, 'law': 1, 'enforcement': 1, 'agency': 1, 'suspect': 1, 'critical': 1, 'information': 1, 'Jodie': 1, 'Bernstein': 1, 'director': 1, 'Federal': 1, 'Trade': 1, 'Commission': 1, 'Bureau': 1, 'Consumer': 1, 'Protection': 1, 'major': 1, 'help': 1, 'FTC': 1, 'identify': 1, 'particular': 1, 'infancy': 1, 'example': 1, 'shut': 1, 'run': 1, 'Alliance': 1, 'take': 1, '6': 1, 'million': 1, 'earn': 1, '5,000': 1, 'month': 1, 'initial': 1, '250': 1, 'instead': 1, 'keep': 1, 'money': 1, 'charge': 1, 'visit': 1, '370,000': 1, 'time': 1, 'forward': 1, 'local': 1, 'state': 1, 'federal': 1, 'authority': 1, 'second': 1, 'sale': 1, 'service': 1, 'custom': 1, 'design': 1, 'access': 1, 'account': 1, 'place': 1, 'crook': 1, 'sell': 1, 'computer': 1, 'equipment': 1, 'memory': 1, 'chip': 1, 'sound': 1, 'board': 1, 'net': 1, 'deliver': 1, 'significantly': 1, 'low': 1, 'quality': 1, 'good': 1, 'involve': 1, 'opportunity': 1, 'con': 1, 'artist': 1, 'offer': 1, 'share': 1, 'franchise': 1, 'unreasonable': 1, 'prediction': 1, 'misrepresentation': 1, 'let': 1, 'rich': 1, 'work': 1, 'home': 1, 'announce': 1, 'Tuesday': 1, 'NationsBank': 1, 'donate': 1, '100,000': 1, 'sponsor': 1})\n",
      "['variety', '$', 'involve', 'Federal', 'directly', 'Alliance', 'set', 'share', 'praise', 'low', 'time', 'scam', 'non', 'rich', 'place', 'opportunity', 'example', 'Commission', 'fund', 'quality', 'Consumer', 'sell', 'track', '250', '100,000', 'February', 'overflow', 'site', 'enforcement', 'commission', 'league', 'law', 'internet', 'computer', 'Consumers', 'instead', 'Bernstein', 'Trade', 'board', 'Internet', 'FTC', 'crime', 'charge', '\\n', 'federal', 'access', 'information', 'shut', 'take', 'advocacy', 'wide', 'misrepresentation', 'Fraud', 'help', 'collect', 'equipment', 'bogus', '6', 'report', 'offer', 'infancy', 'sound', 'Jodie', 'director', 'money', 'custom', 'early', 'critical', 'home', 'investor', 'unreasonable', 'later', 'deliver', 'significantly', 'fashioned', 'promise', 'cyberspace', 'pyramid', 'net', 'Watch', 'prediction', 'old', 'donate', 'http://www.fraud.org', 'initial', 'NationsBank', '370,000', 'Wednesday', 'Fortuna', 'League', 'pay', 'world', 'month', 'service', 'particular', 'work', 'earn', 'consumer', 'major', 'let', 'visit', 'state', 'announce', 'sponsor', '5,000', 'deposit', 'run', 'profit', 'memory', 'technology', 'group', 'second', 'suspect', 'new', 'good', 'widely', 'business', 'web', 'identify', 'call', 'say', 'design', 'National', 'popular', 'con', 'million', 'authority', 'Tuesday', 'keep', 'Bureau', 'sale', 'agency', 'franchise', 'account', 'Protection', 'chip', 'artist', 'local', 'crook', 'scheme', 'forward']\n",
      "Counter({'\\n': 11, 'mail': 9, 'say': 7, 'Post': 7, 'Office': 7, 'electronic': 5, 'internet': 5, 'service': 5, 'Reisner': 5, 'Postal': 4, 'Service': 4, 'plan': 4, ' \\n': 4, 'security': 3, 'use': 3, 'new': 3, 'develop': 3, 'system': 3, 'bulk': 3, 'online': 2, 'commerce': 2, 'enhance': 2, 'business': 2, 'verify': 2, 'e': 2, 'tamper': 2, 'like': 2, 'send': 2, 'register': 2, 'investigate': 2, 'delivery': 2, 'Sun': 2, 'mailer': 2, 'Java': 2, 'private': 2, 'pilot': 2, 'early': 2, 'today': 2, 'U.S.': 1, 'announce': 1, 'Wednesday': 1, 'boost': 1, 'reliability': 1, 'travel': 1, 'consumer': 1, 'available': 1, 'ordinary': 1, 'certify': 1, 'letter': 1, 'leap': 1, 'trading': 1, 'message': 1, 'buy': 1, 'sell': 1, 'good': 1, 'block': 1, 'fear': 1, 'threat': 1, 'Robert': 1, 'vice': 1, 'president': 1, 'stategic': 1, 'planning': 1, 'expand': 1, 'local': 1, 'area': 1, 'network': 1, 'bilateral': 1, 'secure': 1, 'communication': 1, 'wide': 1, 'require': 1, 'generation': 1, 'reisner': 1, 'Cylink': 1, 'Corp': 1, 'identity': 1, 'sender': 1, 'enable': 1, 'people': 1, 'digital': 1, 'signature': 1, 'compare': 1, 'tampering': 1, 'discover': 1, 'regular': 1, 'non': 1, 'conjunction': 1, 'Microsystems': 1, 'rely': 1, 'computer': 1, 'language': 1, 'instead': 1, 'manually': 1, 'calculate': 1, 'postage': 1, 'rate': 1, 'mailing': 1, 'possibly': 1, 'outdate': 1, 'form': 1, 'able': 1, 'add': 1, 'charge': 1, 'software': 1, 'application': 1, 'write': 1, 'post': 1, 'eventually': 1, 'offer': 1, 'hybrid': 1, 'involve': 1, 'kind': 1, 'convert': 1, 'paper': 1, 'deliver': 1, 'day': 1, 'example': 1, 'move': 1, 'change': 1, 'address': 1, 'critic': 1, 'usurp': 1, 'function': 1, 'better': 1, 'perform': 1, 'sector': 1, 'reject': 1, 'argument': 1, 'maintain': 1, 'facilitate': 1, 'spread': 1, 'technology': 1, 'railroad': 1, 'aircraft': 1, 'remember': 1, 'return': 1, 'World': 1, 'War': 1, 'risk': 1, 'life': 1, 'dangerous': 1, 'duty': 1, 'Resiner': 1, 'refer': 1, 'airmail': 1, 'effort': 1, 'dare': 1, 'alive': 1, 'create': 1, 'part': 1, 'know': 1, 'weather': 1, 'navigation': 1, 'aid': 1, 'watch': 1, 'dynamic': 1, 'experience': 1, 'tell': 1, 'public': 1, 'job': 1, '--202': 1, '898': 1, '8312': 1})\n",
      "['available', 'involve', 'alive', 'leap', 'kind', 'network', 'letter', 'trading', 'function', 'job', 'block', 'post', 'non', 'security', 'rely', 'know', 'aid', 'paper', 'language', 'reisner', 'today', 'compare', 'example', 'experience', 'secure', 'airmail', 'mailer', 'identity', 'regular', 'delivery', 'commerce', 'perform', 'conjunction', 'sell', 'Java', 'develop', 'critic', 'return', 'address', 'remember', 'convert', 'require', 'like', 'boost', '8312', 'internet', 'computer', 'private', 'instead', 'mail', 'Cylink', 'online', ' \\n', 'Resiner', 'president', 'charge', 'day', '\\n', 'move', 'wide', 'digital', 'expand', 'Sun', 'application', 'discover', 'fear', 'weather', 'spread', 'World', 'message', 'dynamic', 'Postal', 'offer', 'communication', 'reliability', 'watch', 'enhance', 'Robert', 'tampering', 'dare', 'tell', 'sender', 'bulk', 'maintain', 'Reisner', 'vice', 'early', '898', 'hybrid', 'U.S.', 'form', 'area', 'duty', 'Post', 'enable', 'stategic', '--202', 'calculate', 'deliver', 'use', 'generation', 'create', 'navigation', 'travel', 'threat', 'usurp', 'verify', 'better', 'signature', 'possibly', 'Corp', 'add', 'life', 'effort', 'register', 'outdate', 'Wednesday', 'e', 'mailing', 'service', 'buy', 'software', 'pilot', 'consumer', 'electronic', 'manually', 'send', 'announce', 'part', 'aircraft', 'able', 'reject', 'bilateral', 'system', 'plan', 'certify', 'technology', 'change', 'risk', 'refer', 'facilitate', 'new', 'postage', 'Office', 'good', 'business', 'ordinary', 'Microsystems', 'say', 'tamper', 'public', 'argument', 'investigate', 'rate', 'railroad', 'sector', 'Service', 'write', 'eventually', 'dangerous', 'local', 'people', 'War', 'planning']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-00a5e4a9401a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mnew_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_words\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mword_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# trying to cheat to see if I can do this without straight vectorizing:\n",
    "\n",
    "word_list = []\n",
    "    \n",
    "for dct in df_train['Meaningful Word Count']:\n",
    "    print(dct)\n",
    "    print(list(set(dct)))\n",
    "    new_words = list(set(dct.keys()) - set(word_list))\n",
    "    if list(new_words) != []:\n",
    "        word_list.append(list(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up brief analysis\n",
    "\n",
    "cluster_X_train = df_train[['Raw Character Count', 'Raw Word Count', 'Meaningful Word Count Total']]\n",
    "cluster_X_test = df_test[['Raw Character Count', 'Raw Word Count', 'Meaningful Word Count Total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n",
      "AuthorNum  0   1   2   3   4   5   6   7   8   9  ...  40  41  42  43  44  45  \\\n",
      "row_0                                             ...                           \n",
      "0          71  56  74  64  44  42  27  35  59  41 ...  60  51  73  61  65  59   \n",
      "1           4  19   1  11  31  33  48  40  16  34 ...  15  24   2  14  10  16   \n",
      "\n",
      "AuthorNum  46  47  48  49  \n",
      "row_0                      \n",
      "0          58  38  47  70  \n",
      "1          17  37  28   5  \n",
      "\n",
      "[2 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "y = df_train['AuthorNum']\n",
    "\n",
    "X_norm = normalize(cluster_X_train)\n",
    "X_pca = PCA(2).fit_transform(X_norm)\n",
    "\n",
    "# PCA spreads data out (as a result of lower dimensionality, don't ask)\n",
    "# Normalizing is just needed (don't ask) - it will throw a ValueError otherwise.\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=2, random_state=42).fit_predict(X_pca)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, y)) # pandas version of confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Czech Republic and Spain played out a scoreless draw in their World Cup group six qualifier on Wednesday, in a match that never lived up to expectations.\n",
      "The Czechs were facing their first big test since they reached the Euro 96 final, while Real Madrid's teenage striker Raul was looking to spark a depleted Spanish attack in his first full international.\n",
      "Both sides opened their World Cup campaigns last month with high-scoring victories over the two weakest teams in the group, Spain winning 6-2 in the Faroe Islands and the Czechs thrashing Malta 6-0.\n",
      "But Yugoslavia have already collected three wins and Slovakia two against the same two hapless victims and neither team could afford to give ground in Prague.\n",
      "Like two heavyweights feeling each other out in the early rounds, both teams started tentatively, waiting to pounce on the other's mistakes.\n",
      "The Spaniard's were the first to flinch when Kaiserslautern striker Pavel Kuka's cross found an unmarked Karel Poborsky just outside the crease. But the Manchester United midfielder failed to control the ball, wasting what would turn out be one of the game's few good scoring chances.\n",
      "Next it was the Czechs turn to falter. Newcastle United goalkeeper Pavel Srnicek, winning his first cap in over a year, tried to clear the ball, but hit attacker Alfonso Perez and watched helplessly as it rolled just wide of the net.\n",
      "The Czechs picked up their play in the second half, putting Spain on their heels for the rest of the game.\n",
      "\"I don't think we lost points tonight because they are such an excellent team. They played strongly in the defence and its too bad we missed out on the two great chances we had,\" said Czech striker Patrik Berger.\n",
      "Teams:\n",
      "Czech Republic: 1-Pavel Srnicek, 2-Radoslav Latal, 3-Jan Suchoparek, 4-Pavel Nedved (15-Martin Frydek, 86th), 5-Miroslav Kadlec, 6-Michal Hornak, 7-Jiri Nemec, 8-Karel Poborsky (17-Vladimir Smicer, 58th), 9-Pavel Kuka, 10-Patrik Berger, 11-Radek Bejbl\n",
      "Spain: 1-Andoni Zubizarreta, 2-Abelardo Fernandez, 3-Sergi Barjuan, 4-Rafael Alkorta, 5-Miguel Angel Nadal, 6-Fernando Hierro, 7-Raul Gonzalez, 8-Luis Enrique Martinez, 9-Guillermo Amor (18-Ismael Urzaiz, 76th), 10-Julen Guerrero (14-Josep Guardiola, 52nd), 11-Alfonso Perez (15-Roberto Rios, 73rd)\n",
      "\n",
      "Japan's Finance Ministry pledged on Thursday to lift its controls on management of employee pension funds, a key member of an advisory panel to the prime minister said.\n",
      "At a meeting of the Administrative Reform Council's subcommittee on deregulation, the ministry clearly stated it would abolish restrictions on where employee pension funds managed by trust banks could be invested, Yoshihiko Miyauchi, chairman of the subcommittee, told reporters.\n",
      "Under current rules, each trust bank must invest at least half of the funds it manages in fixed-income assets, such as Japanese government bonds, no more than 30 percent in Japanese stocks, no more than 30 percent in foreign currency-denominated shares and bonds, and no more than 20 percent in real estate.\n",
      "Miyauchi said the Finance Ministry said it would work out details on lifting its regulations on asset allocation by the end of the year.\n",
      "Japan's corporate pension system is divided into employee pension funds, which are supervised by the Welfare Ministry, and Tax-Qualified Retirement Pension (TQP) plans, which are supervised by the Finance Ministry.\n",
      "The Welfare Ministry abolished its restrictions on asset management by trust banks in April.\n",
      "Industry sources currently estimate assets in employee pension funds at about 42 trillion yen ($378 billion) and those in TQPs at about 18 trillion yen ($162 billion).\n",
      "However, Miyauchi quoted Finance Ministry officials as saying the ministry was still considering whether it would allow investment advisory companies to manage TQP plans.\n",
      "At the moment, only trust banks and life insurers are allowed to manage them.\n",
      "The Welfare Ministry has already lifted the rules it had to prevent investment advisory companies managing employee pension funds, although some restrictions remain on the firms.\n",
      "Japanese corporations have been urging the government to reform Japan's rigid corporate pension system in the face of low returns on investment and to help maintain the international competitiveness of Japanese firms.\n",
      "Employers participating in the corporate pension system are required to maintain specified yields, currently 5.5 percent a year, to guarantee pre-determined benefits.\n",
      "However, the actual return on pension funds' investments is now about three percent, due to Japan's record-low interest rates and weak economic recovery, industry sources say.\n",
      "Miyauchi said the Finance Ministry told the sub-committee it was studying ways to relax specified yields but had not given details.\n",
      "Securities industry sources said the abolition of restrictions on asset management would encourage fund managers to pump more money into Tokyo's stock market in order to boost returns on investment. ($1=111)\n",
      "\n",
      "Japan's Finance Ministry, which has long resisted calls from employers to reform the country's rigid corporate pension system, has finally decided to relax its tight grip.\n",
      "However, industry sources say its proposals fall short of what is needed to cope with Japan's rapidly \"greying\" society and the poor financial state of employee pension funds resulting from record-low interest rates and a weak economic recovery.\n",
      "\"Unless Japan undertakes a drastic overhaul of the corporate pension system, it will remain out of step with the rest of the world,\" said an official at an overseas-based investment advisory company.\n",
      "In the past, the amount of money Japanese corporate pension funds held was so small that authorities could regulate them without too much difficulty. But the amount has grown to about 60 trillion yen ($540 billion) and unless the current system is changed, it will hurt corporate balance sheets, he said.\n",
      "Yoshihiko Miyauchi, a key member of an advisory panel to the prime minister, said on Thursday that the Finance Ministry had stated in a subcommittee meeting on deregulation that it would abolish restrictions on where employee pension funds managed by trust banks could be invested.\n",
      "Under the current rules, each trust bank must invest at least half of the funds it manages in fixed-income assets, no more than 30 percent in Japanese stocks, no more than 30 percent in foreign currency-denominated shares and bonds, and no more than 20 percent in real estate.\n",
      "Japan's corporate pension system is divided into employee pension funds, which are supervised by the Welfare Ministry, and Tax-Qualified Retirement Pension plans (TQPs), which are supervised by the Finance Ministry.\n",
      "Miyauchi said the Finance Ministry was still considering whether to allow investment advisory firms to manage TQPs. It also told the subcommittee that it was studying whether to allow variable yields and flexible benefits, he said.\n",
      "The Welfare Ministry has steadily lifted its restrictions on the management on employee pension funds, which have assets estimated at 42 trillion yen ($378 billion), and allowed investment advisory firms to manage the funds.\n",
      "However, the Finance Ministry has barred investment advisory firms from managing TQPs, which have assets totalling 18 trillion yen ($162 billion). Only trust banks and life insurers are allowed to manage TQPs.\n",
      "Employers participating in the corporate pension system are required to maintain specified yields, currently 5.5 percent a year, to guarantee predetermined benefits. But the actual return on investments is now only about three percent.\n",
      "Because of the poor investment environment, many corporate pension funds are facing a shortage of funds to pay benefits to pensioners in the future, industry sources said. If pension funds fall short of meeting future payments, sponsoring companies must cover the shortfall.\n",
      "Industry sources said that investment advisory firms, especially foreign companies, are keen to manage TQPs.\n",
      "\"Generally speaking, fund managers at Japanese companies are conservative,\" said the official with the investment advisory company.\n",
      "\"They tend to avoid investment risks because they see the risks, which are short-term volatility, as something dangerous. But experienced foreign advisors are well-qualified to manage pension funds as they take reasonable risks in long-term pension fund management over 20- to 30-year periods,\" he said.\n",
      "He said British pension funds invest about 90 percent of their assets in equities, including about 15 percent in foreign stocks.\n",
      "Ken Okamura, a strategist at Dresdner Kleinwort Benson (Asia) Ltd, said in a recent report that if Japan completely abolished restrictions on asset management of corporate pension funds, it was likely to see more investment in local stocks.\n",
      "He estimated that at the moment, life insurers and trust banks put an average of about 20 percent of the pension assets they are entrusted with into local stocks.\n",
      "\"If corporate pension funds increase the weighting of local stocks to 30 percent, about six trillion yen ($54.0 billion) of funds will come to Japanese stock markets.\" ($1=111 yen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_train['Spacy-ed Text'][52])\n",
    "print(df_train['Spacy-ed Text'][520])\n",
    "print(df_train['Spacy-ed Text'][521])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['specialist',\n",
       " 'plan',\n",
       " 'telephone',\n",
       " 'interview',\n",
       " 'group',\n",
       " '\\n',\n",
       " 'chair',\n",
       " 'complete',\n",
       " 'Association',\n",
       " 'David',\n",
       " 'aim',\n",
       " 'paper',\n",
       " 'issue',\n",
       " 'say',\n",
       " 'minimize',\n",
       " 'trademark',\n",
       " 'International',\n",
       " 'committee',\n",
       " 'month',\n",
       " 'release',\n",
       " 'work',\n",
       " 'association',\n",
       " 'address',\n",
       " 'name',\n",
       " 'white',\n",
       " 'Trademark',\n",
       " 'internet',\n",
       " 'dispute']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdas = {1, 4, 3, 33, 333, 3333, 1, 3}\n",
    "ada = {1, 3, 4}\n",
    "\n",
    "ee = ['lead', 'co', 'Maher', 'recommendation']\n",
    "dddd = list(set(df_train['Meaningful Word Count'][5].keys()) - set(ee))\n",
    "dddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['specialist',\n",
       " 'co',\n",
       " 'Maher',\n",
       " 'plan',\n",
       " 'telephone',\n",
       " 'interview',\n",
       " 'recommendation',\n",
       " 'chair',\n",
       " 'complete',\n",
       " 'Association',\n",
       " 'David',\n",
       " 'aim',\n",
       " 'paper',\n",
       " 'minimize',\n",
       " 'trademark',\n",
       " 'International',\n",
       " 'committee',\n",
       " 'release',\n",
       " 'work',\n",
       " 'association',\n",
       " 'address',\n",
       " 'white',\n",
       " 'Trademark',\n",
       " 'dispute']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(df_train['Meaningful Word Count'][5].keys()) - set(df_train['Meaningful Word Count'][6].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-19a1261b6289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'hey'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mama'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mddd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mama'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdd\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;34m'mama'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'str'"
     ]
    }
   ],
   "source": [
    "dd = ['hey', 'mama']\n",
    "ddd = ['mama']\n",
    "dd-'mama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 3, 4}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd = [1,3,4]\n",
    "set(asd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
