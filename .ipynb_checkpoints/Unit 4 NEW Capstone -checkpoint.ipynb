{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 Capstone - News Article Analysis & Classification\n",
    "\n",
    "## John A. Fonte\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Find 100 different entries from at least 10 different authors (articles?)\n",
    "2. Reserve 25% for test set\n",
    "3. cluster vectorized data (go through a few clustering methods)\n",
    "4. Perform unsupervised feature generation and selection\n",
    "5. Perform supervised modeling by classifying by author\n",
    "6. Comment on your 25% holdout group. Did the clusters for the holdout group change dramatically, or were they consistent with the training groups? Is the performance of the model consistent? If not, why?\n",
    "7. Conclude with which models (clustering or not) work best for classifying texts.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "__Source:__ https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#\n",
    "\n",
    "__Description:__ This is a subset of the [Reuters Corpus Volume 1 (RCV1)](https://scikit-learn.org/0.17/datasets/rcv1.html). Specifically, this subset consists of the top 50 authors by article proliferation, with a total of 100 articles per each author within the combined training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 1. Data Load and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "# will be doing other imports ad hoc\n",
    "##### i.e., models and related functions\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Data from Local Computer\n",
    "Each author is a subfolder, and within each folder is a series of .txt files\n",
    "The goal of this cell is to load all the contents of every subfolder into the \n",
    "DataFrame, while retaining the author designation for those works.\n",
    "'''\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "def multiple_file_load(file_directory):\n",
    "    \n",
    "    # identifying all author subfolders - appending them into list \n",
    "    \n",
    "    authorlist = []\n",
    "    textlist = []\n",
    "    \n",
    "    for author in listdir(file_directory):\n",
    "        authorname = str(author)\n",
    "        author_sub_directory = (file_directory + '/' + author) #author file path\n",
    "    \n",
    "    # identifying all files within each subfolder - \n",
    "    \n",
    "        for filename in listdir(author_sub_directory):\n",
    "            text_file_path = (author_sub_directory + '/' + filename) # text file path\n",
    "            \n",
    "            if (filename.lower().endswith('txt')):\n",
    "                authorlist.append(authorname)\n",
    "                textfile = open(text_file_path,'r') # this is how you open files\n",
    "                substantive_text = textfile.read()  # this is how to read a file\n",
    "                textlist.append(substantive_text)   # this is how to do something with that file\n",
    "                textfile.close()                    # this is how to close the file \n",
    "                                                             # (you must close one before opening another!)\n",
    "  # pushing the two lists into a dataframe \n",
    "\n",
    "    df = pd.DataFrame({'Author':authorlist, 'Text':textlist})\n",
    "    \n",
    "    return df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data (note the file path)\n",
    "df_train = multiple_file_load('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/C50/C50train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author                                               Text\n",
       "0  AaronPressman  The Internet may be overflowing with new techn...\n",
       "1  AaronPressman  The U.S. Postal Service announced Wednesday a ...\n",
       "2  AaronPressman  Elementary school students with access to the ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors don't have space between the names\n",
    "# adding the space in the authors...because I want it\n",
    "import re\n",
    "author_split = [re.findall('[A-Z][a-z]*', i) for i in df_train.Author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining them back together\n",
    "author_join = []\n",
    "\n",
    "for couple in author_split:\n",
    "    joined_string = couple[0] + ' ' + couple[1]\n",
    "    author_join.append(joined_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China issued tough new rules on the handling o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>China will avoid bold moves in tackling its ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>William Kazer</td>\n",
       "      <td>Communist Party chief Jiang Zemin has put his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author                                               Text\n",
       "2497  William Kazer  China issued tough new rules on the handling o...\n",
       "2498  William Kazer  China will avoid bold moves in tackling its ai...\n",
       "2499  William Kazer  Communist Party chief Jiang Zemin has put his ..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Author'] = pd.Series(author_join)\n",
    "df_train.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>U.S. Senators on Tuesday sharply criticized a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>Two members of Congress criticised the Federal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>Commuters stuck in traffic on the Leesburg Pik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author                                               Text\n",
       "0  AaronPressman  U.S. Senators on Tuesday sharply criticized a ...\n",
       "1  AaronPressman  Two members of Congress criticised the Federal...\n",
       "2  AaronPressman  Commuters stuck in traffic on the Leesburg Pik..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading df_test, which is a separate csv file\n",
    "\n",
    "df_test = multiple_file_load('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/C50/C50test')\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another fix to Author column\n",
    "\n",
    "author_split = [re.findall('[A-Z][a-z]*', i) for i in df_test.Author]\n",
    "\n",
    "author_join = []\n",
    "\n",
    "for couple in author_split:\n",
    "    joined_string = couple[0] + ' ' + couple[1]\n",
    "    author_join.append(joined_string)    \n",
    "    \n",
    "df_test['Author'] = pd.Series(author_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before I begin adding features, assignment asks for 25% data split, NOT 50/50\n",
    "# See \"GOAL\" below for explanation as to how I am doing that.\n",
    "\n",
    "'''GOAL:\n",
    "Trying to get half of the datapoints OF EACH AUTHOR\n",
    "in the testing set into a new DataFrame, which\n",
    "will be concatenated onto the training set.\n",
    "I will delete that from the testing set later.\n",
    "\n",
    "Doing this instead of combining both and splitting 75/25 later \n",
    "ensures balanced data between the authors.\n",
    "'''\n",
    "\n",
    "def appendingdataframe(dataframe):\n",
    "    appendabledataframe = pd.DataFrame(columns=['Author', 'Text'])\n",
    "    \n",
    "    for item in dataframe.Author.unique():\n",
    "        df_testauthor = df_test[df_test['Author'] == item].copy() \n",
    "        appendabledataframe = appendabledataframe.append(df_testauthor[25:], \n",
    "                                                         ignore_index=True) # want half of df_testauthor!\n",
    "    \n",
    "    return appendabledataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through  Aaron Pressman\n",
      "Looping through  Alan Crosby\n",
      "Looping through  Alexander Smith\n",
      "Looping through  Benjamin Kang\n",
      "Looping through  Bernard Hickey\n",
      "Looping through  Brad Dorfman\n",
      "Looping through  Darren Schuettler\n",
      "Looping through  David Lawder\n",
      "Looping through  Edna Fernandes\n",
      "Looping through  Eric Auchard\n",
      "Looping through  Fumiko Fujisaki\n",
      "Looping through  Graham Earnshaw\n",
      "Looping through  Heather Scoffield\n",
      "Looping through  Jane Macartney\n",
      "Looping through  Jan Lopatka\n",
      "Looping through  Jim Gilchrist\n",
      "Looping through  Joe Ortiz\n",
      "Looping through  John Mastrini\n",
      "Looping through  Jonathan Birt\n",
      "Looping through  Jo Winterbottom\n",
      "Looping through  Karl Penhaul\n",
      "Looping through  Keith Weir\n",
      "Looping through  Kevin Drawbaugh\n",
      "Looping through  Kevin Morrison\n",
      "Looping through  Kirstin Ridley\n",
      "Looping through  Kourosh Karimkhany\n",
      "Looping through  Lydia Zajc\n",
      "Looping through  Lynne O\n",
      "Looping through  Lynnley Browning\n",
      "Looping through  Marcel Michelson\n",
      "Looping through  Mark Bendeich\n",
      "Looping through  Martin Wolk\n",
      "Looping through  Matthew Bunce\n",
      "Looping through  Michael Connor\n",
      "Looping through  Mure Dickie\n",
      "Looping through  Nick Louth\n",
      "Looping through  Patricia Commins\n",
      "Looping through  Peter Humphrey\n",
      "Looping through  Pierre Tran\n",
      "Looping through  Robin Sidel\n",
      "Looping through  Roger Fillion\n",
      "Looping through  Samuel Perry\n",
      "Looping through  Sarah Davison\n",
      "Looping through  Scott Hillis\n",
      "Looping through  Simon Cowell\n",
      "Looping through  Tan Ee\n",
      "Looping through  Therese Poletti\n",
      "Looping through  Tim Farrand\n",
      "Looping through  Todd Nissen\n",
      "Looping through  William Kazer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using appendabledataframe to avoid screwing up original data\n",
    "# This is explicit inefficiency at the cost of being cautious\n",
    "\n",
    "df_train2 = df_train.append(appendingdataframe(df_train), ignore_index=True)\n",
    "\n",
    "# checking if the appending worked\n",
    "len(df_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It worked!\n",
    "df_train = df_train2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through  Aaron Pressman\n",
      "Looping through  Alan Crosby\n",
      "Looping through  Alexander Smith\n",
      "Looping through  Benjamin Kang\n",
      "Looping through  Bernard Hickey\n",
      "Looping through  Brad Dorfman\n",
      "Looping through  Darren Schuettler\n",
      "Looping through  David Lawder\n",
      "Looping through  Edna Fernandes\n",
      "Looping through  Eric Auchard\n",
      "Looping through  Fumiko Fujisaki\n",
      "Looping through  Graham Earnshaw\n",
      "Looping through  Heather Scoffield\n",
      "Looping through  Jane Macartney\n",
      "Looping through  Jan Lopatka\n",
      "Looping through  Jim Gilchrist\n",
      "Looping through  Joe Ortiz\n",
      "Looping through  John Mastrini\n",
      "Looping through  Jonathan Birt\n",
      "Looping through  Jo Winterbottom\n",
      "Looping through  Karl Penhaul\n",
      "Looping through  Keith Weir\n",
      "Looping through  Kevin Drawbaugh\n",
      "Looping through  Kevin Morrison\n",
      "Looping through  Kirstin Ridley\n",
      "Looping through  Kourosh Karimkhany\n",
      "Looping through  Lydia Zajc\n",
      "Looping through  Lynne O\n",
      "Looping through  Lynnley Browning\n",
      "Looping through  Marcel Michelson\n",
      "Looping through  Mark Bendeich\n",
      "Looping through  Martin Wolk\n",
      "Looping through  Matthew Bunce\n",
      "Looping through  Michael Connor\n",
      "Looping through  Mure Dickie\n",
      "Looping through  Nick Louth\n",
      "Looping through  Patricia Commins\n",
      "Looping through  Peter Humphrey\n",
      "Looping through  Pierre Tran\n",
      "Looping through  Robin Sidel\n",
      "Looping through  Roger Fillion\n",
      "Looping through  Samuel Perry\n",
      "Looping through  Sarah Davison\n",
      "Looping through  Scott Hillis\n",
      "Looping through  Simon Cowell\n",
      "Looping through  Tan Ee\n",
      "Looping through  Therese Poletti\n",
      "Looping through  Tim Farrand\n",
      "Looping through  Todd Nissen\n",
      "Looping through  William Kazer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing same for df_test\n",
    "\n",
    "df_test2 = df_test.append(appendingdataframe(df_train), ignore_index=True)\n",
    "\n",
    "# checking if the appending worked\n",
    "len(df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now to drop the rows added to df_train from df_test\n",
    "\n",
    "df_test2.drop_duplicates(keep=False, inplace=True)\n",
    "len(df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "Arguably the most important part about working with text data is how to refine it for processing. As simple as they are, string substitutions such as `pd.replace` and regex's `re.sub` are common. I am also partial to `pd.Series.apply(lambda x: x.replace('...',''))`.  Additional text processing such as the exclusion of stop_words and lemmatization will be done after the raw text is pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text before feature analysis/engineering \n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "# CLEANING FUNCTION 1 - WORD AND PUNCTUATION/CHARACTER CLEANING\n",
    "\n",
    "# EDIT: During first run-through, this function was very basic\n",
    "# I have since implemented new cleaning features\n",
    "\n",
    "# regex already imported as re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    text = text.lower() # avoiding capitalization problems.\n",
    "    \n",
    "    text = re.sub(r'.\\s*\\\\n[a-z]', r'\\. [a-z]', text)\n",
    "    text = re.sub(r'\\.\\s?([a-z])', r'\\. \\1', text)\n",
    "    text = re.sub(r' u\\. s\\.(\\s?)', r' u\\.s\\.\\1', text) # next three lines are my attempt to join 'u. s.' to 'u.s.'\n",
    "    text = text.replace('u. s.', 'u.s.')\n",
    "    text = text.replace(r'u. s.', r'u.s.')\n",
    "    text = re.sub(r'-', '', text)\n",
    "    text = re.sub(r'  ', ' ', text)\n",
    "    text = re.sub('[\\[].*?[\\]]', '', text)\n",
    "    text = re.sub('.=.', '. .', text)\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = re.sub(',', '', text) # I don't want punct screwing up lemmatization\n",
    "    text = re.sub('\\\\n', '', text)\n",
    "    text = re.sub(r'\\\\n', '', text) # I don't know which one works\n",
    "    text = text.replace('\\\"', '')\n",
    "    \n",
    "    # rest of punctuation will be handled via lemmatization\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application of text cleaning functions\n",
    "\n",
    "df_train['Text1'] = df_train['Text'].apply(lambda x: text_cleaner(x))\n",
    "df_test['Text1'] = df_test['Text'].apply(lambda x: text_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the u. s. postal service announced wednesday a plan to boost online commerce by enhancing the security and reliability of electronic mail traveling on the internet. under the plan businesses and consumers can verify that email has not been tampered with and use services now available for ordinary mail like sending a certified letter.the leap from trading messages to buying and selling goods has been blocked by the fear of security threats robert reisner vice president of stategic planning said. to expand from local area networks and bilateral secure communications to wide use of electronic commerce will require a new generation of security services reisner said. cylink corp is developing a system for the post office to use to verify the identity of email senders. the system will enable people to register a digital signature with the post office that can be compared against electronic mail they send. if any tampering is discovered the postal service would investigate just like it investigates tampering with regular mail reisner said. the post office is also using the internet to enhance nonelectronic mail delivery. in conjunction with sun microsystems the postal service has also developed a new online system for bulk mailers that relies on sun's java computer language reisner said. instead of manually calculating postage rates for bulk mailings possibly using outdated forms a bulk mailer will be able to add up the charges using a software application written with java and posted on the internet. eventually the post office plans to offer hybrid services involving both kinds of mail reisner said. electronic mail could be converted to paper and delivered within one business day for example. or someone who moves could register a change of address with the post office over the internet. critics of the post office's internet plans have said the service is usurping functions that could be better performed by the private sector. but reisner rejected those arguments maintaining that the post office has often facilitated the spread of new technologies such as railroads and aircraft. remember the pilots who returned from world war i and then risked their lives in an even more dangerous duty resiner said refering to early airmail delivery efforts.to keep these daring pilots alive the postal service had to create parts of what we know as the weather service today. navigation aids had to be developed. we are watching many of these same dynamics today. earlier experience tells us that this is a public and private job.2028988312\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Text1'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rr']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = ['rr']\n",
    "ddd = []\n",
    "\n",
    "dddd = dd + ddd\n",
    "dddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-5b37932597ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdddd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'here is abelly'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdddd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'el'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'remove'"
     ]
    }
   ],
   "source": [
    "dddd = 'here is abelly'\n",
    "print(dddd.remove('el'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING FUNCTION 2 - NUMBER CLEANING\n",
    "'''\n",
    "Whether it be phone numbers, page numbers, or just \n",
    "digits for no particular reason (which, yes, does happen),\n",
    "numbers will become there own vectors and inevitably clog\n",
    "up the vectorized feature space.\n",
    "\n",
    "I am dropping all number columns because \n",
    "we are looking at words, not numbers!!!!!\n",
    "'''\n",
    "\n",
    "def phone_and_weird_num_deletion(text):\n",
    "    final_text = text\n",
    "    \n",
    "    leading_zero_numbers = re.findall(r' 0\\d+/g', text)\n",
    "    phone_numbers = re.findall(r' ?\\+?\\d{1}?(\\d{3}?|\\d{4}?) \\d{3} \\d{4}/g', text)\n",
    "    phone_num_no_space = re.findall(r' ?\\+?(\\d{10}?|\\d{11}?)/g', text)\n",
    "    total_deletions = leading_zero_numbers + phone_numbers + phone_num_no_space\n",
    "    \n",
    "    if len(total_deletions) != 0:\n",
    "        final_text = total_deletions.apply(lambda x: final_text.replace(x, ''))\n",
    "        return final_text\n",
    "    \n",
    "    else:\n",
    "        return text\n",
    "                               \n",
    "#----------------------------------------------\n",
    "        \n",
    "# DON'T USE THE BELOW FUNCTION!\n",
    "# It appears there are a lot of good numbers there\n",
    "\n",
    "#-----------------------------------------------\n",
    "\n",
    "#def string_num_deletion(text):\n",
    "#    final_text = text\n",
    "#    \n",
    "#    dates = [str(x) for x in list(range(1900, 2026))]\n",
    "#    plural_dates = re.findall(r' (\\d{2}?|\\d{4}?)s/g', text)\n",
    "#    money_values = re.findall(r' \\$\\d+/g', text)\n",
    "#    common_num = list(range(1000))\n",
    "#    total_exceptions = dates + plural_dates + money_values + common_num\n",
    "#                              \n",
    "#    \n",
    "#    num_token = re.findall(r'\\d+[^snrt]?[^tdh]?\\.*\\d*/g', text) # finding all numbers that are not ordinals\n",
    "#                                                                # also using \".\" for decimal findings\n",
    "#    if len(num_token) != 0:\n",
    "#        for x in num_token not in total_exceptions:\n",
    "#            final_text = num_token.apply(lambda x: final_text.replace(x, ''))\n",
    "#        \n",
    "#        return final_text\n",
    "#    \n",
    "#    else:\n",
    "#        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Text2'] = df_train['Text1'].apply(lambda x: phone_and_weird_num_deletion(x))\n",
    "df_test['Text2'] = df_test['Text1'].apply(lambda x: phone_and_weird_num_deletion(x))\n",
    "\n",
    "# The list comprehension inside a loop makes this run for a total of 10seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Adding Features\n",
    "\n",
    "Just some fun numerical features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-62e22b195bf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# adding some numerical features for text analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Raw Character Count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Raw Word Count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-62e22b195bf5>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# adding some numerical features for text analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Raw Character Count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Raw Word Count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# adding some numerical features for text analysis\n",
    "\n",
    "df_train['Raw Character Count'] = df_train['Text'].apply(lambda x: len(x))\n",
    "df_train['Raw Word Count'] = df_train['Text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f77af9292890>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# doing same for df_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Raw Character Count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Raw Word Count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-f77af9292890>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# doing same for df_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Raw Character Count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Raw Word Count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# doing same for df_test\n",
    "\n",
    "df_test['Raw Character Count'] = df_test['Text'].apply(lambda x: len(x))\n",
    "df_test['Raw Word Count'] = df_test['Text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating numerical classes for authors:\n",
    "# I feel like one hot encoding would've screwed things up, so I did \"factorize\"\n",
    "\n",
    "df_train['AuthorNum'] = pd.factorize(df_train.Author)[0]\n",
    "df_train['AuthorNum'] = df_train['AuthorNum'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and same for df_test...\n",
    "\n",
    "df_test['AuthorNum'] = pd.factorize(df_test.Author)[0]\n",
    "df_test['AuthorNum'] = df_test['AuthorNum'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing! Changing Text to Numbers\n",
    "\n",
    "Once everything is numerical, then we can feed that data into the clusters for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing tokenization (unigram), lemmatization, and stop_word exclusion\n",
    "# ***AS PART**** of the model, not a separate spacy thing\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Spacy-ed Text'] = df_train['Text'].apply(lambda text: nlp(text))\n",
    "df_test['Spacy-ed Text'] = df_test['Text'].apply(lambda text: nlp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up function to (1) lemmatize AND\n",
    "# (2) exclude stop words from the count\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemmas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_) # this is why we needed to spacy/nlp-ify the texts first\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "df_train['Meaningful Word Count'] = df_train['Spacy-ed Text'].apply(lambda text: lemma_frequencies(text, \n",
    "                                                                                                   include_stop=False))\n",
    "df_test['Meaningful Word Count'] = df_test['Spacy-ed Text'].apply(lambda text: lemma_frequencies(text,\n",
    "                                                                                                include_stop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# didn't want a dictionary. I wanted a total:\n",
    "\n",
    "df_train['Meaningful Word Count Total'] = df_train['Meaningful Word Count'].apply(lambda x: sum(list(x.values())))\n",
    "df_test['Meaningful Word Count Total'] = df_test['Meaningful Word Count'].apply(lambda x: sum(list(x.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemmas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_) # this is why we needed to spacy/nlp-ify the texts first\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Lemmatized Text'] = df_train['Spacy-ed Text'].apply(lambda text: lemmatize(text, include_stop=False))\n",
    "df_test['Lemmatized Text'] = df_test['Spacy-ed Text'].apply(lambda text: lemmatize(text, include_stop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all processing is complete, saving it as a csv so I don't have to do it again\n",
    "\n",
    "\n",
    "# commented out because this cell should only be run once\n",
    "\n",
    "#df_train.to_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/COMPLETE_NLP-train.csv',\n",
    "#                index=False)\n",
    "#\n",
    "#df_test.to_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/COMPLETE_NLP-test.csv',\n",
    "#               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating output for vectors, i.e., \n",
    "# a NEW dataframe from a vectorized \"word matrix\"\n",
    "def word_matrix_2_df(word_matrix, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(word_matrix)]\n",
    "    \n",
    "    df = pd.DataFrame(data=word_matrix.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that tokens are created via lemmatization, we can vectorize those tokens\n",
    "\n",
    "# FIRST VECTORIZER - BAG-OF-WORDS!!!!!!!!!!!\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    tokenizer=dummy, # putting lemmatizer function in here would've thrown an error\n",
    "    preprocessor=dummy,\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwm = cv.fit_transform(list(df_train['Lemmatized Text']))\n",
    "tokens = cv.get_feature_names()\n",
    "\n",
    "df_train_vectorized = word_matrix_2_df(cwm, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------\n",
    "# NEW VECTORIZER - Tfidf!!!!!!!!\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x, ngram_range=(1,1))\n",
    "tfidf_vectors = tfidf.fit_transform(df_train['Lemmatized Text'])\n",
    "\n",
    "# save this commented-out thing for later?\n",
    "# df_word_matrix_TRAIN = pd.DataFrame(tfidf_vectors.todense(), columns=tfidf.vocabulary_)\n",
    "\n",
    "# feature_names the same from previous vectorizer, so feature_names=tokens again\n",
    "# (this is my way of saying that tfidf_vectors.get_feature_names throws an AttributeError)\n",
    "\n",
    "df_train_tfidf_vectorized = word_matrix_2_df(tfidf_vectors, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tfidf_vectorized.iloc[:,100-250].value_counts() # highly sparse data again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING ANALYSIS\n",
    "\n",
    "Looking at which vectorized features are the biggest influencers.\n",
    "\n",
    "Here, we will be using *x\\_train\\_tfidf\\_vectorized* as the input data. Tfidf is a measure of frequency, which means the proportions of the raw counts (found in CountVectorized()) are the same among datapoints, but the frequency character of the dataset naturally allows scaling that accounts for any particular outliers, which would otherwise negatively affect clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = df_train['AuthorNum']\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_pca = PCA(2).fit_transform(df_train_tfidf_vectorized) # gigantic feature space\n",
    "\n",
    "# PCA spreads data out (as a result of lower dimensionality, don't ask)\n",
    "# Normalizing/standardscaling is just needed for this - otherwise, it will throw a ValueError otherwise.\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans).fit(X_pca)\n",
    "\n",
    "# Calculate the cluster labels: THIS IS THE OUTPUT YOU ARE LOOKING AT!!!!!!!!!!!!********\n",
    "labels = pipeline.predict(X_pca)\n",
    "\n",
    "# Create a DataFrame with cluster labels and Classes as columns: df\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x')\n",
    "\n",
    "## Calculate predicted values.\n",
    "#y_pred = KMeans(n_clusters=5, random_state=42).fit_predict(X_pca)\n",
    "\n",
    "# Plot the solution.\n",
    "#plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the crosstab data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem with above is that while cluster points scaled and transformed\n",
    "# cluster centers refused to do so\n",
    "\n",
    "# don't need scaling or normalization because all tfidf values are between 0 and 1 anyway\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "# K-Means, Round 2\n",
    "\n",
    "X_pca = PCA(2).fit_transform(df_train_tfidf_vectorized) \n",
    "\n",
    "kmeans = KMeans(n_clusters=13, n_init=50, max_iter=500000, tol=0.000001)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "#from sklearn.preprocessing import Normalizer\n",
    "#normalizer=Normalizer()\n",
    "\n",
    "#pipeline = make_pipeline(normalizer, kmeans).fit_transform(X_pca)\n",
    "kmeans.fit(X_pca)\n",
    "labels = kmeans.predict(X_pca) # same as kmeans.labels_ --- I just did it this way\n",
    "\n",
    "\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "xcenters = cluster_centers[:,0]\n",
    "ycenters = cluster_centers[:,1]\n",
    "xcenlist = list(xcenters)\n",
    "ycenlist = list(ycenters)\n",
    "\n",
    "legendlabels = range(0,13)\n",
    "\n",
    "centerdf = pd.DataFrame({'Xcenter':xcenters, 'Ycenter':ycenters,\n",
    "                        'Labels':[str(label) for label in legendlabels]})\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(x=X_pca[:, 0], y=X_pca[:, 1], c=labels)\n",
    "plt.legend(centerdf.Labels)\n",
    "\n",
    "# Plotting kMeans centers\n",
    "plt.scatter(x=xcenters, y=ycenters,\n",
    "            c='red', marker='x')\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, len(xcenlist)):\n",
    "    print('Cluster {} has center coordinates of: ({}, {})'.format(str(legendlabels[i]), \n",
    "                                                                  round(xcenlist[i], 2), \n",
    "                                                                  round(ycenlist[i], 2)))\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__K-Means Analysis:__\n",
    "\n",
    "The space in the tfidf is very compact, making separation highly difficult. This is expected when you try to reduce a high-dimensional space via PCA; flattening down thousands of dimensions down to `PCA(2)` overcrowds the data, exemplifying the __curse of dimensionality.__  Originally, a large number of clusters (here, K=13) was hypothesized to provide accurate classification information without overfitting and overcome this curse of dimensionality. However, as the crosstab results show, some of the authors' classifications are spread more or less evenly across multiple clusters. This means the model was unable to use the PCA-ed features to classify those particular authors effectively.\n",
    "\n",
    "So how can we maximize the accuracy of the model? What _is_ the accuracy of a clustering model? This is where an _Elbow Visualizer_ and the _Silhouette score_ come in, respectively.\n",
    "\n",
    "A silhouette score for the above model is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing silhouette score\n",
    "\n",
    "from sklearn.metrics import silhouette_score \n",
    "silhouette_score(X_pca, labels) # silhouette score for k=13 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this Silhouette Score Mean?\n",
    "\n",
    "A Silhouette value is a distance measure of a sample's distance to its cluster's center (a) and its distance to the center of the nearest cluster that the sameple is not a part of (b). A __Silhouette Coefficient__ is the average of all of these values for a given cluster. [As used in sklearn.metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), a __Silhouette Score__ is the average of all Silhouette Coefficients across all clusters.\n",
    "\n",
    "Silhouette Coefficients (and by nature of the metric, Silhouette Scores too) range from (-1,1), with \"1\" being the best score. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
    "\n",
    "In short, a silhouette score is a __measure of how accurate the clustering is.__\n",
    "\n",
    "---\n",
    "\n",
    "__Analysis:__ As you can see, our Silhouette Score for k=13 clusters of 36.5% is not particularly comforting. How do we determine the best number of clusters to obtain the highest accuracy for our dataset?  This is where the __K-Elbow Visualizer__ comes in.\n",
    "\n",
    "The \"Elbow Method\" is a simple iterative method that measures the accuracies for a range of k-sized clustering.  The point of inflection (the \"elbow\" of the graph) is usually the indicator that the model fits best with that point (in this case, that the model fits best with that particular k-number of clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not a good percentage! \n",
    "\n",
    "# Import the KElbowVisualizer method \n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate a scikit-learn K-Means model \n",
    "# really cranking on these iterations, \n",
    "# because I have yet to see any overfitting or significant computational overload...\n",
    "model = KMeans(n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric \n",
    "visualizer = KElbowVisualizer(model, k=(2,21), metric='silhouette', timings=True)\n",
    "\n",
    "# Fit the data and visualize \n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "visualizer.fit(X_pca) \n",
    "visualizer.poof() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# judging by the graph, silhouette score for K=2 is ~58%.\n",
    "# welp, that settles it! going back and doing a final K-Means with K=2\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "# K-Means, Round 3\n",
    "\n",
    "X_pca = PCA(2).fit_transform(df_train_tfidf_vectorized) \n",
    "\n",
    "kmeans = KMeans(n_clusters=2, n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "kmeans.fit(X_pca)\n",
    "labels = kmeans.predict(X_pca)\n",
    "\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "xcenters = cluster_centers[:,0]\n",
    "ycenters = cluster_centers[:,1]\n",
    "xcenlist = list(xcenters)\n",
    "ycenlist = list(ycenters)\n",
    "\n",
    "legendlabels = range(0,2)\n",
    "\n",
    "centerdf = pd.DataFrame({'Xcenter':xcenters, 'Ycenter':ycenters,\n",
    "                        'Labels':[str(label) for label in legendlabels]})\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "#matplotlib thought \"black and white\" were good plotting colors for no reason, so that's neat...\n",
    "\n",
    "color_list = []\n",
    "for value in labels:\n",
    "    if value == 0:\n",
    "        color_list.append('orange')\n",
    "    else:\n",
    "        color_list.append('blue')\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(x=X_pca[:, 0], y=X_pca[:, 1], c=color_list) \n",
    "\n",
    "# Plotting kMeans centers\n",
    "plt.scatter(x=xcenters, y=ycenters,\n",
    "            c='red', marker='x')\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, len(xcenlist)):\n",
    "    print('Cluster {} has center coordinates of: ({}, {})'.format(str(legendlabels[i]), \n",
    "                                                                  round(xcenlist[i], 2), \n",
    "                                                                  round(ycenlist[i], 2)))\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_crosstab_analysis = pd.DataFrame({'Author':df_train['Author'].unique()})\n",
    "\n",
    "cluster_fraction_list = []\n",
    "cluster_final_list = []\n",
    "\n",
    "for i in range(0,50):\n",
    "    cluster0_count = ct[i].iloc[0]\n",
    "    cluster1_count = ct[i].iloc[1]\n",
    "    cluster_total = 75\n",
    "    \n",
    "    if cluster0_count > cluster1_count:\n",
    "        cluster0avg = cluster0_count / cluster_total\n",
    "        cluster_fraction_list.append(cluster0avg)\n",
    "        cluster_final_list.append(0)\n",
    "        \n",
    "    else:\n",
    "        cluster1avg = cluster1_count / cluster_total\n",
    "        cluster_fraction_list.append(cluster1avg)\n",
    "        cluster_final_list.append(1)\n",
    "        \n",
    "\n",
    "df_crosstab_analysis['Cluster No. Classification'] = pd.Series(cluster_final_list)\n",
    "df_crosstab_analysis['Percent of Author in the Cluster Classification'] = pd.Series(cluster_fraction_list)\n",
    "df_crosstab_analysis = df_crosstab_analysis.sort_values(by=['Cluster No. Classification',\n",
    "                                                           'Percent of Author in the Cluster Classification'],\n",
    "                                                        ascending=False)\n",
    "\n",
    "# just did this manually\n",
    "print('The percentage of Authors found to be in Cluster 1 more than in Cluster 0 is 20% (10/50 total Authors).')\n",
    "\n",
    "df_crosstab_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis:__ \n",
    "\n",
    "Again, only 20% of the `df_train` dataset can be characterized as falling into Cluster 1. More so, out of that 20%, only two authors (37 - Peter Humphrey and 45 - Tan Ee) were classified in Cluster 1 by more than 75%. These results come from K=2, which has the highest silhouette score of ~58%..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# So what now?\n",
    "\n",
    "Obtaining the highest silhouette score of ~58% leaves us at a point where there is an expected upper-limit to model accuracy moving forward. Did we do something wrong?\n",
    "\n",
    "I propose yes. Yes we did.\n",
    "\n",
    "Thinkful's Capstone Project instructions appear to place the cart before the horse. According to the instructions (which are listed in the beginning of this project): \"The _first technique_ is to create a series of clusters. ... _Next_, perform some unsupervised feature generation and selection using techniques covered in this unit and elsewhere in the course.\"\n",
    "\n",
    "It is a fact that dimensionality reduction and proper scaling are _required_ for clustering. While not synonymous, dimensionality reduction goes hand in hand with feature selection and generation; the dimensionality reduction algorithm is created \"new\" composite features as projections. From there, we can cluster and perform unsupervised learning.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Table of Contents for Remainder of Project\n",
    "\n",
    "1. Re-vectorize data using tfidf again, but with different (better) hyperparameters.\n",
    "\n",
    "\n",
    "2. Use gensim's `word2vec` to perform unsupervised learning on the dataset (this model has dimensionality reduction built-in as a hyperparameter).\n",
    "\n",
    "\n",
    "3. Reduce dimensionality using the more appropriate dimensionality reduction algorithm, _Latent Semantic Analysis (LSA)_, for the remainder of unsupervised learning analysis.\n",
    "    - retain original `df_train` for aggregate number features. This will be separately clustered and analyzed.\n",
    "    \n",
    "    \n",
    "4. Apply clustering models:\n",
    "    - K-Means (again)\n",
    "    - Spectral Clustering\n",
    "    - t-SNE\n",
    "\n",
    "\n",
    "5. Perform any feature selection, testing whether to include aggregate number features in the vectorized DataFrame.\n",
    "\n",
    "\n",
    "6. Build and fit supervised classification models.\n",
    "\n",
    "\n",
    "7. Test fitted models on vectorized and dimensionality-reduced testing data.\n",
    "\n",
    "\n",
    "8. Apply test data to previously used clustering models.\n",
    "    - Compare and contrast the results with those of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: Load the data\n",
    "# I have since restarted the kernel\n",
    "# luckily, I saved my text analysis df's\n",
    "\n",
    "df_train = pd.read_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/COMPLETE_NLP-train.csv')\n",
    "df_test = pd.read_csv('D:/Github/Data-Science-Bootcamp/CAPSTONE - Unsupervised Learning/COMPLETE_NLP-test.csv')\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not so lucky, it appears that python doesn't know what to do with lemmatized text as that is a spacy datatype\n",
    "# gotta re-clean?...\n",
    "\n",
    "# STEP 1: DATA CLEANING\n",
    "# wanted to do something more comprehensive than the above text cleaner function\n",
    "\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('\\'', ''))\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('[', ''))\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace(']', ''))\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace(',', ''))\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace(r'\\\\n', ''))\n",
    "df_train['Lemmatized Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('.', '')) # tried to do re\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('-', ' '))# but didn't work...\n",
    "df_train['Lemmatized Text'] = df_train['Lemmatized Text'].apply(lambda x: x.replace('\\\\n', ''))\n",
    "df_train['Lemmatized Text'][1]\n",
    "                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's...fine.\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('\\'', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('[', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace(']', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace(',', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('.', ''))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('-', ' '))\n",
    "df_test['Lemmatized Text'] = df_test['Lemmatized Text'].apply(lambda x: x.replace('\\\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - RE-VECTORIZING TRAINING DATA\n",
    "\n",
    "# Instantiate vectorizer model\n",
    "# input here for demonstration purposes - already imported\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word',\n",
    "                        ngram_range=(1,1),      # ensuring unigram nature\n",
    "                        min_df=2,               # only use words that appear at least twice******\n",
    "                        stop_words='english',   # stop words and lowercase were already done \n",
    "                        lowercase=True,         # but it doesn't hurt to do a second time\n",
    "                        use_idf=True,           # um yeah, that's what we're doing...\n",
    "                        norm=u'l2',             # Applies a correction factor for imbalanced sized tokens\n",
    "                        smooth_idf=True         # smooth_idf adds 1 to all document frequencies, \n",
    "                       )                        # as if an extra document existed that used every word once.  \n",
    "                                                # Prevents divide-by-zero errors\n",
    "    \n",
    "tfidf_vectors = tfidf.fit_transform(df_train['Lemmatized Text'])\n",
    "#dense = tfidf_vectors.todense() ----- for a feature space of this size\n",
    "#denselist = dense.tolist()     ------ fixing sparsity will crash/run out of memory\n",
    "\n",
    "token_names = tfidf.get_feature_names()\n",
    "\n",
    "# creating dataframe from vectors (a separate process)\n",
    "vectorized_df_train = pd.DataFrame(tfidf_vectors.toarray(), columns=token_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAFRAME CLEANING\n",
    "'''\n",
    "This vectorized dataframe is extremely sparse.\n",
    "Trying to do tfidf_vectors.todense() threw a memory error. Not risking that.\n",
    "Dropping all number columns because we are looking at words, not numbers.\n",
    "'''\n",
    "import re\n",
    "\n",
    "numbertokens = []\n",
    "dates = [str(x) for x in list(range(1900, 2026))]\n",
    "\n",
    "for name in token_names:\n",
    "    num_token = re.findall(r'\\d+[^t][^h]', name)\n",
    "    num_not_date = [x for x in num_token if x not in dates]\n",
    "    if len(num_not_date) != 0:\n",
    "        numbertokens.append(num_not_date)\n",
    "        \n",
    "print(len(numbertokens))\n",
    "numbertokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbertokens is list of lists. Need to undo that\n",
    "from itertools import chain\n",
    "\n",
    "numbertokens2 = list(chain.from_iterable(numbertokens))\n",
    "numbertokens2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbertokens2[25:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(numbertokens2))\n",
    "print(len(list(set(numbertokens2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates, there are none\n",
    "print(len(token_names))\n",
    "print(len(set(token_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING to drop all numerical tokens\n",
    "\n",
    "for item in numbertokens2 and token_names:\n",
    "    vectorized_df_train.drop(columns=[item], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I give up, just manually doing it:\n",
    "vectorized_df_train = vectorized_df_train.iloc[:,1808:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis and Dimensionality Reduction via LSA\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a dimensionality reduction method that measures the angles of the vectors via a cosine algorithm. This cosine algorithm essentially creates a normal range between 0 and 1.\n",
    "\n",
    "For efficiency purposes, dimensionality reduction is crucial for unsupervised learning methods such as clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from nearly 20000 to 150\n",
    "svd= TruncatedSVD(150)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(vectorized_df_train) ###### LSA IS THE NAME OF THE MODEL USED HERE!\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of authors our LSA model considers similar, for the first (not top) ***FIVE*** components\n",
    "components=pd.DataFrame(X_train_lsa,index=df_train['Author'])\n",
    "for i in range(5):\n",
    "    print('\\nComponent {}:\\n'.format(i))\n",
    "    print(components.loc[:,i].sort_values(ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this tell us?\n",
    "\n",
    "Tells us that no one writes like David Lawder!\n",
    "\n",
    "It is disheartening, however, to see that the truncation resulted in the retention of only 37% variance. While this is somewhat expected given that we are flattening nearly 20000 features into 150, this is still a significant loss and could play a role in making our future models inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if LSA can tell us \n",
    "\n",
    "# running LSA again with 50 components, like 50 clusters:\n",
    "svd= TruncatedSVD(50)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(vectorized_df_train) ###### LSA IS THE NAME OF THE MODEL USED HERE!\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "components=pd.DataFrame(X_train_lsa,index=df_train['Author'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsort = {}\n",
    "for column in components.columns:\n",
    "    columndict = {str(column):components[column].max()}\n",
    "    columnsort.update(columndict)\n",
    "    \n",
    "columnsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components 0,1,3,7 had highest numbers:\n",
    "\n",
    "for i in [0,1,3,7]:\n",
    "    print('\\nComponent {}:\\n'.format(i))\n",
    "    print(components.loc[:,i].sort_values(ascending=False)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above to figure out which components best describe a given author.\n",
    "\n",
    "To do this, we could use `pd.idmax(axis=0)`\n",
    "\n",
    "# Clustering\n",
    "\n",
    "Using the above, we have an idea of what kinds of clusterings we expect from authors' writings. Unforntuately, we have to use reduce dimensionality down to 2 if we want to visualize the clusters. SVD is still preferable over PCA because PCA requires a central tendency, and there is no such thing in a sparse vector matrix. On the other hand, PCA retains variance, and we have seen the huge drops in variance explainability with SVD/LSA.\n",
    "\n",
    "Let's try SVD first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-doing SVD to get 2 dimensions\n",
    "svd= TruncatedSVD(2)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(vectorized_df_train)\n",
    "\n",
    "# K-Means clustering\n",
    "y = df_train['Author']\n",
    "kmeans = KMeans(n_clusters=5, n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "kmeans.fit(X_train_lsa)\n",
    "labels = kmeans.predict(X_train_lsa)\n",
    "\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "xcenters = cluster_centers[:,0]\n",
    "ycenters = cluster_centers[:,1]\n",
    "xcenlist = list(xcenters)\n",
    "ycenlist = list(ycenters)\n",
    "\n",
    "centerdf = pd.DataFrame({'Xcenter':xcenters, 'Ycenter':ycenters})\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(x=X_train_lsa[:, 0], y=X_train_lsa[:, 1], c=labels) \n",
    "\n",
    "# Plotting kMeans centers\n",
    "plt.scatter(x=xcenters, y=ycenters,\n",
    "            c='red', marker='x')\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, len(xcenlist)):\n",
    "    print('Cluster {} has center coordinates of: ({}, {})'.format(i, \n",
    "                                                                  round(xcenlist[i], 2), \n",
    "                                                                  round(ycenlist[i], 2)))\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, creating a central tendency for graphing purposes might be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_pca = PCA(2).fit_transform(vectorized_df_train) \n",
    "\n",
    "kmeans = KMeans(n_clusters=5, n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans).fit(X_pca)\n",
    "\n",
    "# Calculate the cluster labels\n",
    "labels = pipeline.predict(X_pca)\n",
    "\n",
    "dfkmeans = pd.DataFrame({'Cluster Label':labels, 'Author':y})\n",
    "ct = pd.crosstab(dfkmeans['Cluster Label'], dfkmeans['Author'])\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "xcenters = cluster_centers[:,0]\n",
    "ycenters = cluster_centers[:,1]\n",
    "xcenlist = list(xcenters)\n",
    "ycenlist = list(ycenters)\n",
    "\n",
    "centerdf = pd.DataFrame({'Xcenter':xcenters, 'Ycenter':ycenters})\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Plotting kMeans\n",
    "plt.scatter(x=X_pca[:, 0], y=X_pca[:, 1], c=labels) \n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i in range(5):\n",
    "    print('Cluster {} has center coordinates of: ({}, {})'.format(i, \n",
    "                                                                  round(xcenlist[i], 2), \n",
    "                                                                  round(ycenlist[i], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing silhouette score\n",
    "\n",
    "from sklearn.metrics import silhouette_score \n",
    "silhouette_score(X_pca, labels) # silhouette score for k=5 clusters, above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing one more KElbow analysis\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate a scikit-learn K-Means model \n",
    "# really cranking on these iterations, \n",
    "# because I have yet to see any overfitting or significant computational overload...\n",
    "model = KMeans(n_init=500, max_iter=500000, tol=0.000001, random_state=42)\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric \n",
    "visualizer = KElbowVisualizer(model, k=(2,21), metric='silhouette', timings=True)\n",
    "\n",
    "# Fit the data and visualize \n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "visualizer.fit(X_pca) \n",
    "visualizer.poof() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do\n",
    "\n",
    "run mean shift with PCA\n",
    "run spectral OR affinity with PCA\n",
    "run t-SNE\n",
    "---\n",
    "create feature spaces, incorporating top components from LSA and aggregate numbers from original\n",
    "DO THE SAME FOR XTEST\n",
    "run supervised classification models\n",
    "- lasso/ridge\n",
    "- knn\n",
    "- tree model/boosting model\n",
    "unsupervised modelling on testing dataset\n",
    "explain classification analyis and compare/contrast models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SciPy hierarchical clustering doesn't fit into a sklearn pipeline, so we need to use the \n",
    "# normalize() function from sklearn.preprocessing instead of Normalizer.\n",
    "\n",
    "# Import normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "# Normalize the movements: normalized_movements\n",
    "normalized_movements = normalize(sample_data)\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(normalized_movements, method='complete')\n",
    "\n",
    "\n",
    "# Plot the dendrogram, using varieties as labels\n",
    "dendrogram(mergings,labels=varieties, leaf_rotation=90,leaf_font_size=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing around with the K ---- checking elbow for best number of clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
